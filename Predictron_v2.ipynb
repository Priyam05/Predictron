{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Distributions and Modules\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import distributions\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Normalization(df):\n",
    "    df.iloc[:,2:]= df.iloc[:,2:].apply(lambda x: ((x-x.mean()) / (x.std())))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(path):\n",
    "    df = pd.read_csv(path, header=None, delimiter=' ')\n",
    "    \n",
    "    #Normalize the data\n",
    "    df = Normalization(df)\n",
    "    \n",
    "    #Drop the columns which has all values as Nan\n",
    "    df.dropna(axis=1, how='all', inplace=True)\n",
    "    \n",
    "    #Get Rewards for each time step : 0 except last time step where reward is -100\n",
    "    df['Counter'] = df.index\n",
    "    lastRowIndex = df.groupby(0).last().Counter.tolist()\n",
    "    df['reward'] = df['Counter'].apply(lambda x : -100 if x in lastRowIndex else 0 )\n",
    "    df.drop(columns=['Counter'],inplace=True)\n",
    "    \n",
    "    #Rename columns\n",
    "    df.rename(columns={0: \"machine\", 1: \"time\"}, inplace=True)\n",
    "    \n",
    "    #Calculate Monte Carlo Value for each row\n",
    "    df1 = df.groupby('machine').last()[['time']].reset_index()\n",
    "    df = pd.merge(df, df1, on = 'machine', how = 'left').rename(columns ={'time_x':'time','time_y':'lastTimeStamp'})\n",
    "    df['MC_Val'] = (gamma ** (df['lastTimeStamp'] - df['time'] )) * (-100)\n",
    "    df = df.drop(columns='lastTimeStamp')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = preprocessing(\"/home/abc/Berkeley/Prof_Ram/CMAPSSData/train_FD001.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We have added the reward and Val column. We will be using the Val column for the Monte Carlo return gamma**(T-t)  X  -100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now lets build the Neural network for the predictron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Neural network for Observation - Hidden State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN_OH(nn.Module):\n",
    "    def __init__(self, input_size, out_size):\n",
    "        super(NN_OH,self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size,32)\n",
    "        self.fc2 = nn.Linear(32,16)\n",
    "        self.fc3 = nn.Linear(16,out_size)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN_OH(\n",
      "  (fc1): Linear(in_features=24, out_features=32, bias=True)\n",
      "  (fc2): Linear(in_features=32, out_features=16, bias=True)\n",
      "  (fc3): Linear(in_features=16, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "input_size  = 24\n",
    "out_size = 4\n",
    "net = NN_OH(input_size, out_size)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Neural network for Hidden State - Reward & Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN_reward_val(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(NN_reward_val,self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size,32)\n",
    "        self.fc2 = nn.Linear(32,16)\n",
    "        self.fc3 = nn.Linear(16,2)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Neural Network which will take my current hidden state to the next hidden state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN_HH(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(NN_HH,self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size,32)\n",
    "        self.fc2 = nn.Linear(32,32)\n",
    "        self.fc3 = nn.Linear(32,input_size)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we have all the required neural networks for the predictron. Lets build the Predictron\n",
    "\n",
    "class Predictronv2(nn.Module):\n",
    "    def __init__(self, obs_size, hid_size, k=10):\n",
    "        super(Predictronv2,self).__init__()\n",
    "        \n",
    "        #Instantiate Neural Network for Observation-Hidden State\n",
    "        self.fc1 = NN_OH(obs_size, hid_size)\n",
    "        \n",
    "        #Instantiate Neural Network for Hidden State - Reward, Value\n",
    "        self.fc2 = NN_reward_val(hid_size)\n",
    "        \n",
    "        #Instantiate Neural Network for Hidden State - Next Hidden State\n",
    "        self.fc3 = NN_HH(hid_size)\n",
    "        \n",
    "        #K-step return\n",
    "        self.k = k\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #Predictron core will output the value estimate for the current observation. We will input x (observation) \n",
    "        #and get value estimate. This implementation is for a k-step return which can be extended to TD(lambda) return\n",
    "        \n",
    "        #First step: Get the Hidden state for the current observation\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        \n",
    "        #Unlike v1, we want to keep track of all the k-step returns till k and then take average\n",
    "        reward = self.fc2(x)[:,0]\n",
    "        all_k_vals = torch.zeros((x.shape[0],k+1))\n",
    "        all_k_vals[:,0] = reward #This will be the 0-step return\n",
    "        for i in range(1, self.k+1):\n",
    "            #Take the next step\n",
    "            x = self.fc3(x)\n",
    "            val_ith = (gamma**i)*self.fc2(x)[:,1]\n",
    "            #print(val_ith.shape, all_k_vals[:,i].shape, reward.shape)\n",
    "            all_k_vals[:,i] = reward + val_ith \n",
    "            reward += (gamma**(i))*(self.fc2(x))[:,0]      \n",
    "\n",
    "        return (torch.sum(all_k_vals, dim=1)/self.k).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getXY(data):\n",
    "    x = torch.tensor(data.iloc[:, 2:-2].values).float()\n",
    "    y_target = torch.tensor(data.iloc[:,-1].values).float()\n",
    "    y_target = y_target.reshape(-1,1)\n",
    "    \n",
    "    return x, y_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y_target = getXY(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20631, 1])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the loss function and Initialising the Predictron core\n",
    "k=10\n",
    "loss_fn = nn.MSELoss()\n",
    "core = Predictronv2(x.shape[1], 4, k)\n",
    "optimizer = optim.Adam(core.parameters(), lr = 1e-3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after:0 iterations is :tensor(167.8832, grad_fn=<MseLossBackward>)\n",
      "Loss after:1 iterations is :tensor(32.1465, grad_fn=<MseLossBackward>)\n",
      "Loss after:2 iterations is :tensor(21.4750, grad_fn=<MseLossBackward>)\n",
      "Loss after:3 iterations is :tensor(18.1959, grad_fn=<MseLossBackward>)\n",
      "Loss after:4 iterations is :tensor(38.5323, grad_fn=<MseLossBackward>)\n",
      "Loss after:5 iterations is :tensor(26.2335, grad_fn=<MseLossBackward>)\n",
      "Loss after:6 iterations is :tensor(18.2914, grad_fn=<MseLossBackward>)\n",
      "Loss after:7 iterations is :tensor(21.7259, grad_fn=<MseLossBackward>)\n",
      "Loss after:8 iterations is :tensor(55.7215, grad_fn=<MseLossBackward>)\n",
      "Loss after:9 iterations is :tensor(20.2670, grad_fn=<MseLossBackward>)\n",
      "Loss after:10 iterations is :tensor(11.9268, grad_fn=<MseLossBackward>)\n",
      "Loss after:11 iterations is :tensor(31.0279, grad_fn=<MseLossBackward>)\n",
      "Loss after:12 iterations is :tensor(25.4044, grad_fn=<MseLossBackward>)\n",
      "Loss after:13 iterations is :tensor(43.8682, grad_fn=<MseLossBackward>)\n",
      "Loss after:14 iterations is :tensor(27.0942, grad_fn=<MseLossBackward>)\n",
      "Loss after:15 iterations is :tensor(11.4985, grad_fn=<MseLossBackward>)\n",
      "Loss after:16 iterations is :tensor(30.7075, grad_fn=<MseLossBackward>)\n",
      "Loss after:17 iterations is :tensor(29.8529, grad_fn=<MseLossBackward>)\n",
      "Loss after:18 iterations is :tensor(23.6279, grad_fn=<MseLossBackward>)\n",
      "Loss after:19 iterations is :tensor(45.5937, grad_fn=<MseLossBackward>)\n",
      "Loss after:20 iterations is :tensor(59.2717, grad_fn=<MseLossBackward>)\n",
      "Loss after:21 iterations is :tensor(35.1606, grad_fn=<MseLossBackward>)\n",
      "Loss after:22 iterations is :tensor(39.5852, grad_fn=<MseLossBackward>)\n",
      "Loss after:23 iterations is :tensor(16.1787, grad_fn=<MseLossBackward>)\n",
      "Loss after:24 iterations is :tensor(44.6957, grad_fn=<MseLossBackward>)\n",
      "Loss after:25 iterations is :tensor(19.4568, grad_fn=<MseLossBackward>)\n",
      "Loss after:26 iterations is :tensor(49.6605, grad_fn=<MseLossBackward>)\n",
      "Loss after:27 iterations is :tensor(29.2442, grad_fn=<MseLossBackward>)\n",
      "Loss after:28 iterations is :tensor(21.2630, grad_fn=<MseLossBackward>)\n",
      "Loss after:29 iterations is :tensor(7.2075, grad_fn=<MseLossBackward>)\n",
      "Loss after:30 iterations is :tensor(12.5338, grad_fn=<MseLossBackward>)\n",
      "Loss after:31 iterations is :tensor(14.3708, grad_fn=<MseLossBackward>)\n",
      "Loss after:32 iterations is :tensor(19.6446, grad_fn=<MseLossBackward>)\n",
      "Loss after:33 iterations is :tensor(8.3326, grad_fn=<MseLossBackward>)\n",
      "Loss after:34 iterations is :tensor(20.0752, grad_fn=<MseLossBackward>)\n",
      "Loss after:35 iterations is :tensor(40.6267, grad_fn=<MseLossBackward>)\n",
      "Loss after:36 iterations is :tensor(32.4254, grad_fn=<MseLossBackward>)\n",
      "Loss after:37 iterations is :tensor(15.9754, grad_fn=<MseLossBackward>)\n",
      "Loss after:38 iterations is :tensor(25.7542, grad_fn=<MseLossBackward>)\n",
      "Loss after:39 iterations is :tensor(22.2628, grad_fn=<MseLossBackward>)\n",
      "Loss after:40 iterations is :tensor(35.5156, grad_fn=<MseLossBackward>)\n",
      "Loss after:41 iterations is :tensor(19.3771, grad_fn=<MseLossBackward>)\n",
      "Loss after:42 iterations is :tensor(15.6942, grad_fn=<MseLossBackward>)\n",
      "Loss after:43 iterations is :tensor(41.5976, grad_fn=<MseLossBackward>)\n",
      "Loss after:44 iterations is :tensor(18.7435, grad_fn=<MseLossBackward>)\n",
      "Loss after:45 iterations is :tensor(15.4348, grad_fn=<MseLossBackward>)\n",
      "Loss after:46 iterations is :tensor(13.3452, grad_fn=<MseLossBackward>)\n",
      "Loss after:47 iterations is :tensor(23.2600, grad_fn=<MseLossBackward>)\n",
      "Loss after:48 iterations is :tensor(12.7635, grad_fn=<MseLossBackward>)\n",
      "Loss after:49 iterations is :tensor(22.0884, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 50 # or whatever\n",
    "batch_size = 256# or whatever\n",
    "losses=[]\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    # x is our input\n",
    "    permutation = torch.randperm(x.size()[0])\n",
    "\n",
    "    for i in range(0,x.size()[0], batch_size):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        indices = permutation[i:i+batch_size]\n",
    "        batch_x, batch_y = x[indices], y_target[indices]\n",
    "\n",
    "        # in case you wanted a semi-full example\n",
    "        outputs = core.forward(batch_x)\n",
    "        loss = loss_fn(outputs,batch_y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    losses.append(loss.item())\n",
    "    print(\"Loss after:\"+str(epoch)+\" iterations is :\"+ str(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Training Loss for Predictron with 10-step return')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(losses)\n",
    "plt.xlabel(\"No. of iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss for Predictron with \"+str(k)+\"-step return\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 21])\n",
      "torch.Size([32])\n",
      "torch.Size([16, 32])\n",
      "torch.Size([16])\n",
      "torch.Size([4, 16])\n",
      "torch.Size([4])\n",
      "torch.Size([32, 4])\n",
      "torch.Size([32])\n",
      "torch.Size([16, 32])\n",
      "torch.Size([16])\n",
      "torch.Size([2, 16])\n",
      "torch.Size([2])\n",
      "torch.Size([32, 4])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 32])\n",
      "torch.Size([32])\n",
      "torch.Size([4, 32])\n",
      "torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "for p in core.parameters():\n",
    "    print(p.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = preprocessing(\"/home/abc/Berkeley/Prof_Ram/CMAPSSData/test_FD001.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test, y_test = getXY(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13096, 21])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13096, 1])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_test = core.forward(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(332.4039, grad_fn=<MseLossBackward>)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn(predict_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>...</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.730242</td>\n",
       "      <td>-0.694615</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.561019</td>\n",
       "      <td>-0.959213</td>\n",
       "      <td>-0.827618</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.175653</td>\n",
       "      <td>0.972392</td>\n",
       "      <td>-0.364968</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>-0.286877</td>\n",
       "      <td>0.540139</td>\n",
       "      <td>0.684565</td>\n",
       "      <td>-1.186405</td>\n",
       "      <td>-0.291078</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>-0.463388</td>\n",
       "      <td>0.476412</td>\n",
       "      <td>0.793594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.549865</td>\n",
       "      <td>0.325687</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>-0.062579</td>\n",
       "      <td>-1.063145</td>\n",
       "      <td>-0.100976</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.175653</td>\n",
       "      <td>-0.480742</td>\n",
       "      <td>-1.235415</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>-0.797297</td>\n",
       "      <td>-0.603480</td>\n",
       "      <td>-0.544922</td>\n",
       "      <td>-0.987163</td>\n",
       "      <td>1.029187</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>-0.463388</td>\n",
       "      <td>0.335250</td>\n",
       "      <td>0.490456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.413667</td>\n",
       "      <td>-0.014413</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>-0.885728</td>\n",
       "      <td>-0.117764</td>\n",
       "      <td>-1.442123</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.175653</td>\n",
       "      <td>1.926470</td>\n",
       "      <td>-0.539057</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>-1.154590</td>\n",
       "      <td>0.450794</td>\n",
       "      <td>-0.193640</td>\n",
       "      <td>-0.783014</td>\n",
       "      <td>-0.491013</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.347263</td>\n",
       "      <td>1.464549</td>\n",
       "      <td>-0.203788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-0.267320</td>\n",
       "      <td>-0.354514</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>-0.386850</td>\n",
       "      <td>1.237349</td>\n",
       "      <td>-0.685579</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.175653</td>\n",
       "      <td>-0.715592</td>\n",
       "      <td>-0.016789</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>-0.644171</td>\n",
       "      <td>0.558008</td>\n",
       "      <td>0.860206</td>\n",
       "      <td>-0.809514</td>\n",
       "      <td>-0.635794</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.347263</td>\n",
       "      <td>0.335250</td>\n",
       "      <td>0.729400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.276251</td>\n",
       "      <td>-1.034715</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>-0.561457</td>\n",
       "      <td>-1.133099</td>\n",
       "      <td>-0.673617</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.175653</td>\n",
       "      <td>0.003636</td>\n",
       "      <td>0.679568</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>-0.235835</td>\n",
       "      <td>0.915389</td>\n",
       "      <td>0.157642</td>\n",
       "      <td>-1.010719</td>\n",
       "      <td>-0.866755</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.347263</td>\n",
       "      <td>0.476412</td>\n",
       "      <td>0.508288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2.138835</td>\n",
       "      <td>-1.714917</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.012253</td>\n",
       "      <td>-0.963210</td>\n",
       "      <td>0.498577</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.175653</td>\n",
       "      <td>1.221920</td>\n",
       "      <td>0.157300</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>-0.440003</td>\n",
       "      <td>0.361449</td>\n",
       "      <td>-0.017999</td>\n",
       "      <td>-0.964589</td>\n",
       "      <td>-1.066690</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>-1.274039</td>\n",
       "      <td>-0.864631</td>\n",
       "      <td>0.237247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-0.312719</td>\n",
       "      <td>0.325687</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>-0.985504</td>\n",
       "      <td>-0.289651</td>\n",
       "      <td>-0.661656</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.175653</td>\n",
       "      <td>0.869645</td>\n",
       "      <td>0.331389</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>-0.388961</td>\n",
       "      <td>0.129151</td>\n",
       "      <td>-0.896204</td>\n",
       "      <td>-1.149109</td>\n",
       "      <td>0.812016</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>-0.463388</td>\n",
       "      <td>-0.158819</td>\n",
       "      <td>0.681849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.003856</td>\n",
       "      <td>1.686090</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>-1.359663</td>\n",
       "      <td>1.229354</td>\n",
       "      <td>-0.515132</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.175653</td>\n",
       "      <td>-0.289927</td>\n",
       "      <td>-0.016789</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>-1.869178</td>\n",
       "      <td>0.164889</td>\n",
       "      <td>-0.017999</td>\n",
       "      <td>-0.398270</td>\n",
       "      <td>0.729284</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>-0.463388</td>\n",
       "      <td>-0.441143</td>\n",
       "      <td>0.171865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.640663</td>\n",
       "      <td>0.325687</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>-1.309775</td>\n",
       "      <td>-0.189717</td>\n",
       "      <td>-0.990588</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.175653</td>\n",
       "      <td>0.576083</td>\n",
       "      <td>0.157300</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.019375</td>\n",
       "      <td>1.147687</td>\n",
       "      <td>-0.017999</td>\n",
       "      <td>-0.571012</td>\n",
       "      <td>-0.160085</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>-0.463388</td>\n",
       "      <td>0.899900</td>\n",
       "      <td>0.313329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-1.129903</td>\n",
       "      <td>1.345990</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.785514</td>\n",
       "      <td>-0.475529</td>\n",
       "      <td>-0.562977</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.175653</td>\n",
       "      <td>0.502692</td>\n",
       "      <td>0.331389</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>-0.082709</td>\n",
       "      <td>0.057675</td>\n",
       "      <td>0.508924</td>\n",
       "      <td>-0.408085</td>\n",
       "      <td>-0.511696</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>-1.274039</td>\n",
       "      <td>1.393968</td>\n",
       "      <td>0.845900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>-0.267320</td>\n",
       "      <td>1.345990</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.261692</td>\n",
       "      <td>-1.374941</td>\n",
       "      <td>-0.870977</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.175653</td>\n",
       "      <td>0.972392</td>\n",
       "      <td>0.157300</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>-0.950423</td>\n",
       "      <td>0.075544</td>\n",
       "      <td>-0.193640</td>\n",
       "      <td>-0.867422</td>\n",
       "      <td>-0.808153</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.347263</td>\n",
       "      <td>-0.582306</td>\n",
       "      <td>0.231303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>-0.403517</td>\n",
       "      <td>1.345990</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.461243</td>\n",
       "      <td>0.240002</td>\n",
       "      <td>0.362519</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.175653</td>\n",
       "      <td>-0.906408</td>\n",
       "      <td>0.505478</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.070417</td>\n",
       "      <td>-0.228230</td>\n",
       "      <td>1.211488</td>\n",
       "      <td>-0.918459</td>\n",
       "      <td>0.084664</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>-0.463388</td>\n",
       "      <td>0.758737</td>\n",
       "      <td>0.672339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>-0.494315</td>\n",
       "      <td>0.665788</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.087085</td>\n",
       "      <td>0.066116</td>\n",
       "      <td>0.109839</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.175653</td>\n",
       "      <td>-0.333961</td>\n",
       "      <td>-0.016789</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.172500</td>\n",
       "      <td>0.486532</td>\n",
       "      <td>0.157642</td>\n",
       "      <td>-1.854799</td>\n",
       "      <td>0.536242</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.347263</td>\n",
       "      <td>-0.370562</td>\n",
       "      <td>-0.541400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.095873</td>\n",
       "      <td>1.005889</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.261692</td>\n",
       "      <td>1.499177</td>\n",
       "      <td>0.915723</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.175653</td>\n",
       "      <td>-0.612845</td>\n",
       "      <td>1.027746</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.172500</td>\n",
       "      <td>-0.817909</td>\n",
       "      <td>0.157642</td>\n",
       "      <td>-1.197202</td>\n",
       "      <td>-1.566530</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>1.157913</td>\n",
       "      <td>0.899900</td>\n",
       "      <td>0.839956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1.140054</td>\n",
       "      <td>0.325687</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>-0.411794</td>\n",
       "      <td>-0.933230</td>\n",
       "      <td>0.522499</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.175653</td>\n",
       "      <td>0.576083</td>\n",
       "      <td>1.201836</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.019375</td>\n",
       "      <td>-0.317575</td>\n",
       "      <td>-0.193640</td>\n",
       "      <td>-0.887051</td>\n",
       "      <td>0.288046</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.347263</td>\n",
       "      <td>-0.511725</td>\n",
       "      <td>1.594827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.186672</td>\n",
       "      <td>-1.374816</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.735627</td>\n",
       "      <td>-0.613439</td>\n",
       "      <td>0.428305</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.175653</td>\n",
       "      <td>0.326555</td>\n",
       "      <td>1.027746</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.274584</td>\n",
       "      <td>-0.281837</td>\n",
       "      <td>0.684565</td>\n",
       "      <td>-1.171683</td>\n",
       "      <td>-0.039434</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>-0.463388</td>\n",
       "      <td>-0.582306</td>\n",
       "      <td>0.639053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>-0.358118</td>\n",
       "      <td>-1.034715</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.062141</td>\n",
       "      <td>-1.644764</td>\n",
       "      <td>-0.368608</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.175653</td>\n",
       "      <td>0.047670</td>\n",
       "      <td>0.679568</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.938130</td>\n",
       "      <td>0.093413</td>\n",
       "      <td>-0.193640</td>\n",
       "      <td>-0.994034</td>\n",
       "      <td>-0.263500</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>-0.463388</td>\n",
       "      <td>-0.794049</td>\n",
       "      <td>-0.795797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.867659</td>\n",
       "      <td>-0.014413</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.386412</td>\n",
       "      <td>1.235350</td>\n",
       "      <td>0.096383</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.175653</td>\n",
       "      <td>-0.289927</td>\n",
       "      <td>1.201836</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.529794</td>\n",
       "      <td>-1.389719</td>\n",
       "      <td>-0.017999</td>\n",
       "      <td>-1.165794</td>\n",
       "      <td>-0.501355</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.347263</td>\n",
       "      <td>-1.005793</td>\n",
       "      <td>2.216556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.686062</td>\n",
       "      <td>1.005889</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.336524</td>\n",
       "      <td>-0.973204</td>\n",
       "      <td>-0.023229</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.175653</td>\n",
       "      <td>-0.877051</td>\n",
       "      <td>-0.016789</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>-0.031667</td>\n",
       "      <td>-0.049540</td>\n",
       "      <td>1.387129</td>\n",
       "      <td>-0.420844</td>\n",
       "      <td>0.591397</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>-1.274039</td>\n",
       "      <td>-0.723468</td>\n",
       "      <td>-1.328368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>-0.993706</td>\n",
       "      <td>1.005889</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>-0.486625</td>\n",
       "      <td>0.128075</td>\n",
       "      <td>0.419334</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.175653</td>\n",
       "      <td>-0.744948</td>\n",
       "      <td>0.331389</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.274584</td>\n",
       "      <td>-0.478397</td>\n",
       "      <td>0.157642</td>\n",
       "      <td>-0.730013</td>\n",
       "      <td>0.691365</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>-0.463388</td>\n",
       "      <td>-1.288118</td>\n",
       "      <td>-0.745869</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          2         3         5         6         7         8         9   \\\n",
       "20  1.730242 -0.694615  0.999962  0.561019 -0.959213 -0.827618  0.999962   \n",
       "21  0.549865  0.325687  0.999962 -0.062579 -1.063145 -0.100976  0.999962   \n",
       "22  0.413667 -0.014413  0.999962 -0.885728 -0.117764 -1.442123  0.999962   \n",
       "23 -0.267320 -0.354514  0.999962 -0.386850  1.237349 -0.685579  0.999962   \n",
       "24  1.276251 -1.034715  0.999962 -0.561457 -1.133099 -0.673617  0.999962   \n",
       "25  2.138835 -1.714917  0.999962  0.012253 -0.963210  0.498577  0.999962   \n",
       "26 -0.312719  0.325687  0.999962 -0.985504 -0.289651 -0.661656  0.999962   \n",
       "27  1.003856  1.686090  0.999962 -1.359663  1.229354 -0.515132  0.999962   \n",
       "28  0.640663  0.325687  0.999962 -1.309775 -0.189717 -0.990588  0.999962   \n",
       "29 -1.129903  1.345990  0.999962  0.785514 -0.475529 -0.562977  0.999962   \n",
       "30 -0.267320  1.345990  0.999962  0.261692 -1.374941 -0.870977  0.999962   \n",
       "31 -0.403517  1.345990  0.999962  0.461243  0.240002  0.362519  0.999962   \n",
       "32 -0.494315  0.665788  0.999962  0.087085  0.066116  0.109839  0.999962   \n",
       "33  0.095873  1.005889  0.999962  0.261692  1.499177  0.915723  0.999962   \n",
       "34  1.140054  0.325687  0.999962 -0.411794 -0.933230  0.522499  0.999962   \n",
       "35  0.186672 -1.374816  0.999962  0.735627 -0.613439  0.428305  0.999962   \n",
       "36 -0.358118 -1.034715  0.999962  0.062141 -1.644764 -0.368608  0.999962   \n",
       "37  0.867659 -0.014413  0.999962  0.386412  1.235350  0.096383  0.999962   \n",
       "38  0.686062  1.005889  0.999962  0.336524 -0.973204 -0.023229  0.999962   \n",
       "39 -0.993706  1.005889  0.999962 -0.486625  0.128075  0.419334  0.999962   \n",
       "\n",
       "          10        11        12  ...        14        15        16        17  \\\n",
       "20  0.175653  0.972392 -0.364968  ...  0.999962 -0.286877  0.540139  0.684565   \n",
       "21  0.175653 -0.480742 -1.235415  ...  0.999962 -0.797297 -0.603480 -0.544922   \n",
       "22  0.175653  1.926470 -0.539057  ...  0.999962 -1.154590  0.450794 -0.193640   \n",
       "23  0.175653 -0.715592 -0.016789  ...  0.999962 -0.644171  0.558008  0.860206   \n",
       "24  0.175653  0.003636  0.679568  ...  0.999962 -0.235835  0.915389  0.157642   \n",
       "25  0.175653  1.221920  0.157300  ...  0.999962 -0.440003  0.361449 -0.017999   \n",
       "26  0.175653  0.869645  0.331389  ...  0.999962 -0.388961  0.129151 -0.896204   \n",
       "27  0.175653 -0.289927 -0.016789  ...  0.999962 -1.869178  0.164889 -0.017999   \n",
       "28  0.175653  0.576083  0.157300  ...  0.999962  0.019375  1.147687 -0.017999   \n",
       "29  0.175653  0.502692  0.331389  ...  0.999962 -0.082709  0.057675  0.508924   \n",
       "30  0.175653  0.972392  0.157300  ...  0.999962 -0.950423  0.075544 -0.193640   \n",
       "31  0.175653 -0.906408  0.505478  ...  0.999962  0.070417 -0.228230  1.211488   \n",
       "32  0.175653 -0.333961 -0.016789  ...  0.999962  0.172500  0.486532  0.157642   \n",
       "33  0.175653 -0.612845  1.027746  ...  0.999962  0.172500 -0.817909  0.157642   \n",
       "34  0.175653  0.576083  1.201836  ...  0.999962  0.019375 -0.317575 -0.193640   \n",
       "35  0.175653  0.326555  1.027746  ...  0.999962  0.274584 -0.281837  0.684565   \n",
       "36  0.175653  0.047670  0.679568  ...  0.999962  0.938130  0.093413 -0.193640   \n",
       "37  0.175653 -0.289927  1.201836  ...  0.999962  0.529794 -1.389719 -0.017999   \n",
       "38  0.175653 -0.877051 -0.016789  ...  0.999962 -0.031667 -0.049540  1.387129   \n",
       "39  0.175653 -0.744948  0.331389  ...  0.999962  0.274584 -0.478397  0.157642   \n",
       "\n",
       "          18        19        20        21        24        25  \n",
       "20 -1.186405 -0.291078  0.999962 -0.463388  0.476412  0.793594  \n",
       "21 -0.987163  1.029187  0.999962 -0.463388  0.335250  0.490456  \n",
       "22 -0.783014 -0.491013  0.999962  0.347263  1.464549 -0.203788  \n",
       "23 -0.809514 -0.635794  0.999962  0.347263  0.335250  0.729400  \n",
       "24 -1.010719 -0.866755  0.999962  0.347263  0.476412  0.508288  \n",
       "25 -0.964589 -1.066690  0.999962 -1.274039 -0.864631  0.237247  \n",
       "26 -1.149109  0.812016  0.999962 -0.463388 -0.158819  0.681849  \n",
       "27 -0.398270  0.729284  0.999962 -0.463388 -0.441143  0.171865  \n",
       "28 -0.571012 -0.160085  0.999962 -0.463388  0.899900  0.313329  \n",
       "29 -0.408085 -0.511696  0.999962 -1.274039  1.393968  0.845900  \n",
       "30 -0.867422 -0.808153  0.999962  0.347263 -0.582306  0.231303  \n",
       "31 -0.918459  0.084664  0.999962 -0.463388  0.758737  0.672339  \n",
       "32 -1.854799  0.536242  0.999962  0.347263 -0.370562 -0.541400  \n",
       "33 -1.197202 -1.566530  0.999962  1.157913  0.899900  0.839956  \n",
       "34 -0.887051  0.288046  0.999962  0.347263 -0.511725  1.594827  \n",
       "35 -1.171683 -0.039434  0.999962 -0.463388 -0.582306  0.639053  \n",
       "36 -0.994034 -0.263500  0.999962 -0.463388 -0.794049 -0.795797  \n",
       "37 -1.165794 -0.501355  0.999962  0.347263 -1.005793  2.216556  \n",
       "38 -0.420844  0.591397  0.999962 -1.274039 -0.723468 -1.328368  \n",
       "39 -0.730013  0.691365  0.999962 -0.463388 -1.288118 -0.745869  \n",
       "\n",
       "[20 rows x 21 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.iloc[20:40, 2:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>...</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>reward</th>\n",
       "      <th>MC_Val</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>machine</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31</td>\n",
       "      <td>-0.267320</td>\n",
       "      <td>1.345990</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.261692</td>\n",
       "      <td>-1.374941</td>\n",
       "      <td>-0.870977</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.175653</td>\n",
       "      <td>0.972392</td>\n",
       "      <td>...</td>\n",
       "      <td>0.075544</td>\n",
       "      <td>-0.193640</td>\n",
       "      <td>-0.867422</td>\n",
       "      <td>-0.808153</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.347263</td>\n",
       "      <td>-0.582306</td>\n",
       "      <td>0.231303</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>49</td>\n",
       "      <td>0.822260</td>\n",
       "      <td>-0.354514</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.186860</td>\n",
       "      <td>-0.301643</td>\n",
       "      <td>0.911237</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.175653</td>\n",
       "      <td>-0.348639</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.013802</td>\n",
       "      <td>0.333283</td>\n",
       "      <td>-1.182480</td>\n",
       "      <td>0.849935</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>-1.274039</td>\n",
       "      <td>-0.582306</td>\n",
       "      <td>-0.879011</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>126</td>\n",
       "      <td>-0.721311</td>\n",
       "      <td>1.345990</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>1.010010</td>\n",
       "      <td>0.329943</td>\n",
       "      <td>2.116325</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.175653</td>\n",
       "      <td>-1.713704</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.639885</td>\n",
       "      <td>1.211488</td>\n",
       "      <td>-0.734921</td>\n",
       "      <td>-0.480672</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>1.968564</td>\n",
       "      <td>0.264669</td>\n",
       "      <td>-0.733981</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>106</td>\n",
       "      <td>0.549865</td>\n",
       "      <td>1.345990</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.760570</td>\n",
       "      <td>1.285318</td>\n",
       "      <td>0.320655</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.175653</td>\n",
       "      <td>-1.640314</td>\n",
       "      <td>...</td>\n",
       "      <td>0.236365</td>\n",
       "      <td>0.684565</td>\n",
       "      <td>-0.520956</td>\n",
       "      <td>1.294619</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>1.968564</td>\n",
       "      <td>-2.205674</td>\n",
       "      <td>-0.922996</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>98</td>\n",
       "      <td>-0.585114</td>\n",
       "      <td>-1.374816</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>-0.511569</td>\n",
       "      <td>0.367918</td>\n",
       "      <td>2.186597</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.175653</td>\n",
       "      <td>-0.686236</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.336111</td>\n",
       "      <td>1.387129</td>\n",
       "      <td>-1.296332</td>\n",
       "      <td>0.356990</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>1.157913</td>\n",
       "      <td>-1.005793</td>\n",
       "      <td>0.902961</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>97</td>\n",
       "      <td>-0.267320</td>\n",
       "      <td>1.005889</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>-0.436738</td>\n",
       "      <td>0.555795</td>\n",
       "      <td>-1.016006</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.175653</td>\n",
       "      <td>0.341233</td>\n",
       "      <td>...</td>\n",
       "      <td>0.986865</td>\n",
       "      <td>-1.071845</td>\n",
       "      <td>0.912017</td>\n",
       "      <td>-0.511696</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>-1.274039</td>\n",
       "      <td>0.476412</td>\n",
       "      <td>1.484271</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>134</td>\n",
       "      <td>0.595264</td>\n",
       "      <td>-0.354514</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.286636</td>\n",
       "      <td>-1.027168</td>\n",
       "      <td>0.924694</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.175653</td>\n",
       "      <td>0.429301</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.299706</td>\n",
       "      <td>-0.193640</td>\n",
       "      <td>1.622615</td>\n",
       "      <td>0.832699</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>1.968564</td>\n",
       "      <td>-1.993930</td>\n",
       "      <td>-0.480772</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>121</td>\n",
       "      <td>0.776860</td>\n",
       "      <td>0.325687</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.511131</td>\n",
       "      <td>2.280666</td>\n",
       "      <td>1.604985</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.175653</td>\n",
       "      <td>-0.466064</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.389052</td>\n",
       "      <td>0.333283</td>\n",
       "      <td>0.730442</td>\n",
       "      <td>-0.080800</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>1.157913</td>\n",
       "      <td>-0.935212</td>\n",
       "      <td>0.297875</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>97</td>\n",
       "      <td>2.138835</td>\n",
       "      <td>-0.014413</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>-1.185055</td>\n",
       "      <td>-0.613439</td>\n",
       "      <td>-1.010025</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.175653</td>\n",
       "      <td>1.456770</td>\n",
       "      <td>...</td>\n",
       "      <td>0.129151</td>\n",
       "      <td>-0.896204</td>\n",
       "      <td>1.122056</td>\n",
       "      <td>-0.880543</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>-1.274039</td>\n",
       "      <td>0.405831</td>\n",
       "      <td>0.282421</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>198</td>\n",
       "      <td>0.595264</td>\n",
       "      <td>1.005889</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>1.184617</td>\n",
       "      <td>2.702390</td>\n",
       "      <td>3.028364</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.175653</td>\n",
       "      <td>-1.875164</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.211028</td>\n",
       "      <td>-0.369281</td>\n",
       "      <td>7.429102</td>\n",
       "      <td>2.221907</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>2.779215</td>\n",
       "      <td>-1.358699</td>\n",
       "      <td>-1.786046</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         time         2         3         5         6         7         8  \\\n",
       "machine                                                                     \n",
       "1          31 -0.267320  1.345990  0.999962  0.261692 -1.374941 -0.870977   \n",
       "2          49  0.822260 -0.354514  0.999962  0.186860 -0.301643  0.911237   \n",
       "3         126 -0.721311  1.345990  0.999962  1.010010  0.329943  2.116325   \n",
       "4         106  0.549865  1.345990  0.999962  0.760570  1.285318  0.320655   \n",
       "5          98 -0.585114 -1.374816  0.999962 -0.511569  0.367918  2.186597   \n",
       "...       ...       ...       ...       ...       ...       ...       ...   \n",
       "96         97 -0.267320  1.005889  0.999962 -0.436738  0.555795 -1.016006   \n",
       "97        134  0.595264 -0.354514  0.999962  0.286636 -1.027168  0.924694   \n",
       "98        121  0.776860  0.325687  0.999962  0.511131  2.280666  1.604985   \n",
       "99         97  2.138835 -0.014413  0.999962 -1.185055 -0.613439 -1.010025   \n",
       "100       198  0.595264  1.005889  0.999962  1.184617  2.702390  3.028364   \n",
       "\n",
       "                9        10        11  ...        16        17        18  \\\n",
       "machine                                ...                                 \n",
       "1        0.999962  0.175653  0.972392  ...  0.075544 -0.193640 -0.867422   \n",
       "2        0.999962  0.175653 -0.348639  ... -0.013802  0.333283 -1.182480   \n",
       "3        0.999962  0.175653 -1.713704  ... -1.639885  1.211488 -0.734921   \n",
       "4        0.999962  0.175653 -1.640314  ...  0.236365  0.684565 -0.520956   \n",
       "5        0.999962  0.175653 -0.686236  ... -1.336111  1.387129 -1.296332   \n",
       "...           ...       ...       ...  ...       ...       ...       ...   \n",
       "96       0.999962  0.175653  0.341233  ...  0.986865 -1.071845  0.912017   \n",
       "97       0.999962  0.175653  0.429301  ... -0.299706 -0.193640  1.622615   \n",
       "98       0.999962  0.175653 -0.466064  ... -0.389052  0.333283  0.730442   \n",
       "99       0.999962  0.175653  1.456770  ...  0.129151 -0.896204  1.122056   \n",
       "100      0.999962  0.175653 -1.875164  ... -1.211028 -0.369281  7.429102   \n",
       "\n",
       "               19        20        21        24        25  reward  MC_Val  \n",
       "machine                                                                    \n",
       "1       -0.808153  0.999962  0.347263 -0.582306  0.231303    -100  -100.0  \n",
       "2        0.849935  0.999962 -1.274039 -0.582306 -0.879011    -100  -100.0  \n",
       "3       -0.480672  0.999962  1.968564  0.264669 -0.733981    -100  -100.0  \n",
       "4        1.294619  0.999962  1.968564 -2.205674 -0.922996    -100  -100.0  \n",
       "5        0.356990  0.999962  1.157913 -1.005793  0.902961    -100  -100.0  \n",
       "...           ...       ...       ...       ...       ...     ...     ...  \n",
       "96      -0.511696  0.999962 -1.274039  0.476412  1.484271    -100  -100.0  \n",
       "97       0.832699  0.999962  1.968564 -1.993930 -0.480772    -100  -100.0  \n",
       "98      -0.080800  0.999962  1.157913 -0.935212  0.297875    -100  -100.0  \n",
       "99      -0.880543  0.999962 -1.274039  0.405831  0.282421    -100  -100.0  \n",
       "100      2.221907  0.999962  2.779215 -1.358699 -1.786046    -100  -100.0  \n",
       "\n",
       "[100 rows x 24 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.groupby('machine').last()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.1381e+01],\n",
       "        [-3.4868e+01],\n",
       "        [-3.8742e+01],\n",
       "        [-4.3047e+01],\n",
       "        [-4.7830e+01],\n",
       "        [-5.3144e+01],\n",
       "        [-5.9049e+01],\n",
       "        [-6.5610e+01],\n",
       "        [-7.2900e+01],\n",
       "        [-8.1000e+01],\n",
       "        [-9.0000e+01],\n",
       "        [-1.0000e+02],\n",
       "        [-8.1914e-12],\n",
       "        [-9.1015e-12],\n",
       "        [-1.0113e-11],\n",
       "        [-1.1236e-11],\n",
       "        [-1.2485e-11],\n",
       "        [-1.3872e-11],\n",
       "        [-1.5414e-11],\n",
       "        [-1.7126e-11]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_target[180:200,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -34.8678],\n",
       "        [ -38.7421],\n",
       "        [ -43.0467],\n",
       "        [ -47.8297],\n",
       "        [ -53.1441],\n",
       "        [ -59.0490],\n",
       "        [ -65.6100],\n",
       "        [ -72.9000],\n",
       "        [ -81.0000],\n",
       "        [ -90.0000],\n",
       "        [-100.0000],\n",
       "        [  -0.6363],\n",
       "        [  -0.7070],\n",
       "        [  -0.7855],\n",
       "        [  -0.8728],\n",
       "        [  -0.9698],\n",
       "        [  -1.0775],\n",
       "        [  -1.1973],\n",
       "        [  -1.3303],\n",
       "        [  -1.4781]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[20:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2264],\n",
       "        [ 0.2998],\n",
       "        [ 0.3861],\n",
       "        [ 0.1885],\n",
       "        [ 0.2651],\n",
       "        [ 0.2337],\n",
       "        [ 0.3156],\n",
       "        [ 0.2278],\n",
       "        [ 0.3633],\n",
       "        [ 0.2175],\n",
       "        [ 0.1726],\n",
       "        [ 0.1614],\n",
       "        [ 0.1628],\n",
       "        [ 0.1604],\n",
       "        [ 0.1678],\n",
       "        [ 0.1082],\n",
       "        [ 0.1769],\n",
       "        [-0.0904],\n",
       "        [ 0.1387],\n",
       "        [ 0.1523]], grad_fn=<ViewBackward>)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "core.forward(x_test[20:40,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accessing the trained Predictron Core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Predictronv2(\n",
       "  (fc1): NN_OH(\n",
       "    (fc1): Linear(in_features=21, out_features=32, bias=True)\n",
       "    (fc2): Linear(in_features=32, out_features=16, bias=True)\n",
       "    (fc3): Linear(in_features=16, out_features=4, bias=True)\n",
       "  )\n",
       "  (fc2): NN_reward_val(\n",
       "    (fc1): Linear(in_features=4, out_features=32, bias=True)\n",
       "    (fc2): Linear(in_features=32, out_features=16, bias=True)\n",
       "    (fc3): Linear(in_features=16, out_features=2, bias=True)\n",
       "  )\n",
       "  (fc3): NN_HH(\n",
       "    (fc1): Linear(in_features=4, out_features=32, bias=True)\n",
       "    (fc2): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (fc3): Linear(in_features=32, out_features=4, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.4155, -0.0080,  1.0000,  1.7179,  1.7757,  2.0294,  1.0000,  0.1417,\n",
      "        -2.3926,  3.1464, -1.4501,  1.0000,  2.6539, -1.8080,  3.1125, -1.7206,\n",
      "         1.8439,  1.0000,  1.8010, -1.8605, -3.0005]) tensor([-100.])\n"
     ]
    }
   ],
   "source": [
    "#Sample\n",
    "print(x[191],y_target[191])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-5.2730,  3.7290,  4.8088, -1.1056], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "hidden_state = core.fc1(x[191])\n",
    "print(hidden_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-8.0630, -0.6005], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reward_sample, Reward, Value\n",
    "core.fc2(hidden_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-3.4359,  6.8554,  6.5401, -3.4522], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Next hidden State\n",
    "core.fc3(hidden_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-61.3991]], grad_fn=<ViewBackward>)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Net output\n",
    "core.forward(x[191].reshape(1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>...</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>reward</th>\n",
       "      <th>MC_Val</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>machine</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>192</td>\n",
       "      <td>0.415519</td>\n",
       "      <td>-0.008022</td>\n",
       "      <td>0.999976</td>\n",
       "      <td>1.717950</td>\n",
       "      <td>1.775667</td>\n",
       "      <td>2.029443</td>\n",
       "      <td>0.999976</td>\n",
       "      <td>0.14168</td>\n",
       "      <td>-2.392645</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.807964</td>\n",
       "      <td>3.112500</td>\n",
       "      <td>-1.720613</td>\n",
       "      <td>1.843870</td>\n",
       "      <td>0.999976</td>\n",
       "      <td>1.801015</td>\n",
       "      <td>-1.860455</td>\n",
       "      <td>-3.000487</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>287</td>\n",
       "      <td>-0.224536</td>\n",
       "      <td>2.039326</td>\n",
       "      <td>0.999976</td>\n",
       "      <td>2.337884</td>\n",
       "      <td>2.932057</td>\n",
       "      <td>2.433861</td>\n",
       "      <td>0.999976</td>\n",
       "      <td>0.14168</td>\n",
       "      <td>-1.929416</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.174039</td>\n",
       "      <td>1.583000</td>\n",
       "      <td>1.667906</td>\n",
       "      <td>2.515780</td>\n",
       "      <td>0.999976</td>\n",
       "      <td>3.092369</td>\n",
       "      <td>-2.137086</td>\n",
       "      <td>-1.892875</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>179</td>\n",
       "      <td>0.552673</td>\n",
       "      <td>-0.008022</td>\n",
       "      <td>0.999976</td>\n",
       "      <td>1.657956</td>\n",
       "      <td>2.328582</td>\n",
       "      <td>2.143880</td>\n",
       "      <td>0.999976</td>\n",
       "      <td>0.14168</td>\n",
       "      <td>-1.646960</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.553673</td>\n",
       "      <td>1.443954</td>\n",
       "      <td>5.849562</td>\n",
       "      <td>1.691891</td>\n",
       "      <td>0.999976</td>\n",
       "      <td>3.092369</td>\n",
       "      <td>-2.303064</td>\n",
       "      <td>-3.080856</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>189</td>\n",
       "      <td>-1.093181</td>\n",
       "      <td>-0.690471</td>\n",
       "      <td>0.999976</td>\n",
       "      <td>3.697739</td>\n",
       "      <td>3.520854</td>\n",
       "      <td>2.623848</td>\n",
       "      <td>0.999976</td>\n",
       "      <td>0.14168</td>\n",
       "      <td>-1.624363</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.133364</td>\n",
       "      <td>0.887772</td>\n",
       "      <td>6.063442</td>\n",
       "      <td>2.198489</td>\n",
       "      <td>0.999976</td>\n",
       "      <td>3.092369</td>\n",
       "      <td>-2.081760</td>\n",
       "      <td>-1.746918</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>269</td>\n",
       "      <td>1.009855</td>\n",
       "      <td>1.015652</td>\n",
       "      <td>0.999976</td>\n",
       "      <td>1.557967</td>\n",
       "      <td>1.102058</td>\n",
       "      <td>2.422750</td>\n",
       "      <td>0.999976</td>\n",
       "      <td>0.14168</td>\n",
       "      <td>-2.934961</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.106248</td>\n",
       "      <td>0.887772</td>\n",
       "      <td>3.616934</td>\n",
       "      <td>2.099836</td>\n",
       "      <td>0.999976</td>\n",
       "      <td>2.446692</td>\n",
       "      <td>-2.026434</td>\n",
       "      <td>-2.402801</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>336</td>\n",
       "      <td>-1.413208</td>\n",
       "      <td>-1.714145</td>\n",
       "      <td>0.999976</td>\n",
       "      <td>2.117907</td>\n",
       "      <td>1.415213</td>\n",
       "      <td>2.576073</td>\n",
       "      <td>0.999976</td>\n",
       "      <td>0.14168</td>\n",
       "      <td>-2.923663</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.028215</td>\n",
       "      <td>2.000136</td>\n",
       "      <td>0.014535</td>\n",
       "      <td>1.601236</td>\n",
       "      <td>0.999976</td>\n",
       "      <td>2.446692</td>\n",
       "      <td>-2.524369</td>\n",
       "      <td>-2.437905</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>202</td>\n",
       "      <td>0.644110</td>\n",
       "      <td>0.333203</td>\n",
       "      <td>0.999976</td>\n",
       "      <td>2.217897</td>\n",
       "      <td>3.284357</td>\n",
       "      <td>2.052775</td>\n",
       "      <td>0.999976</td>\n",
       "      <td>0.14168</td>\n",
       "      <td>-2.053697</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.038456</td>\n",
       "      <td>0.748727</td>\n",
       "      <td>6.284136</td>\n",
       "      <td>2.281145</td>\n",
       "      <td>0.999976</td>\n",
       "      <td>2.446692</td>\n",
       "      <td>-2.911652</td>\n",
       "      <td>-1.582485</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>156</td>\n",
       "      <td>3.524355</td>\n",
       "      <td>-1.031695</td>\n",
       "      <td>0.999976</td>\n",
       "      <td>0.678061</td>\n",
       "      <td>2.160587</td>\n",
       "      <td>2.580518</td>\n",
       "      <td>0.999976</td>\n",
       "      <td>0.14168</td>\n",
       "      <td>-2.087592</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.526556</td>\n",
       "      <td>2.278227</td>\n",
       "      <td>-1.128776</td>\n",
       "      <td>2.363800</td>\n",
       "      <td>0.999976</td>\n",
       "      <td>1.801015</td>\n",
       "      <td>-2.690347</td>\n",
       "      <td>-1.625902</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>185</td>\n",
       "      <td>-0.864590</td>\n",
       "      <td>-1.372920</td>\n",
       "      <td>0.999976</td>\n",
       "      <td>2.497867</td>\n",
       "      <td>1.287994</td>\n",
       "      <td>1.402819</td>\n",
       "      <td>0.999976</td>\n",
       "      <td>0.14168</td>\n",
       "      <td>-3.081838</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.929989</td>\n",
       "      <td>2.000136</td>\n",
       "      <td>-0.850418</td>\n",
       "      <td>2.675758</td>\n",
       "      <td>0.999976</td>\n",
       "      <td>2.446692</td>\n",
       "      <td>-1.805129</td>\n",
       "      <td>-0.892421</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>200</td>\n",
       "      <td>-1.458927</td>\n",
       "      <td>-1.714145</td>\n",
       "      <td>0.999976</td>\n",
       "      <td>2.337884</td>\n",
       "      <td>1.607673</td>\n",
       "      <td>2.578295</td>\n",
       "      <td>0.999976</td>\n",
       "      <td>0.14168</td>\n",
       "      <td>-2.912364</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.865515</td>\n",
       "      <td>2.278227</td>\n",
       "      <td>-0.336688</td>\n",
       "      <td>1.638564</td>\n",
       "      <td>0.999976</td>\n",
       "      <td>1.801015</td>\n",
       "      <td>-2.469043</td>\n",
       "      <td>-2.194027</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         time         2         3         5         6         7         8  \\\n",
       "machine                                                                     \n",
       "1         192  0.415519 -0.008022  0.999976  1.717950  1.775667  2.029443   \n",
       "2         287 -0.224536  2.039326  0.999976  2.337884  2.932057  2.433861   \n",
       "3         179  0.552673 -0.008022  0.999976  1.657956  2.328582  2.143880   \n",
       "4         189 -1.093181 -0.690471  0.999976  3.697739  3.520854  2.623848   \n",
       "5         269  1.009855  1.015652  0.999976  1.557967  1.102058  2.422750   \n",
       "...       ...       ...       ...       ...       ...       ...       ...   \n",
       "96        336 -1.413208 -1.714145  0.999976  2.117907  1.415213  2.576073   \n",
       "97        202  0.644110  0.333203  0.999976  2.217897  3.284357  2.052775   \n",
       "98        156  3.524355 -1.031695  0.999976  0.678061  2.160587  2.580518   \n",
       "99        185 -0.864590 -1.372920  0.999976  2.497867  1.287994  1.402819   \n",
       "100       200 -1.458927 -1.714145  0.999976  2.337884  1.607673  2.578295   \n",
       "\n",
       "                9       10        11  ...        16        17        18  \\\n",
       "machine                               ...                                 \n",
       "1        0.999976  0.14168 -2.392645  ... -1.807964  3.112500 -1.720613   \n",
       "2        0.999976  0.14168 -1.929416  ... -2.174039  1.583000  1.667906   \n",
       "3        0.999976  0.14168 -1.646960  ... -2.553673  1.443954  5.849562   \n",
       "4        0.999976  0.14168 -1.624363  ... -2.133364  0.887772  6.063442   \n",
       "5        0.999976  0.14168 -2.934961  ... -2.106248  0.887772  3.616934   \n",
       "...           ...      ...       ...  ...       ...       ...       ...   \n",
       "96       0.999976  0.14168 -2.923663  ... -3.028215  2.000136  0.014535   \n",
       "97       0.999976  0.14168 -2.053697  ... -2.038456  0.748727  6.284136   \n",
       "98       0.999976  0.14168 -2.087592  ... -2.526556  2.278227 -1.128776   \n",
       "99       0.999976  0.14168 -3.081838  ... -1.929989  2.000136 -0.850418   \n",
       "100      0.999976  0.14168 -2.912364  ... -2.865515  2.278227 -0.336688   \n",
       "\n",
       "               19        20        21        24        25  reward  MC_Val  \n",
       "machine                                                                    \n",
       "1        1.843870  0.999976  1.801015 -1.860455 -3.000487    -100  -100.0  \n",
       "2        2.515780  0.999976  3.092369 -2.137086 -1.892875    -100  -100.0  \n",
       "3        1.691891  0.999976  3.092369 -2.303064 -3.080856    -100  -100.0  \n",
       "4        2.198489  0.999976  3.092369 -2.081760 -1.746918    -100  -100.0  \n",
       "5        2.099836  0.999976  2.446692 -2.026434 -2.402801    -100  -100.0  \n",
       "...           ...       ...       ...       ...       ...     ...     ...  \n",
       "96       1.601236  0.999976  2.446692 -2.524369 -2.437905    -100  -100.0  \n",
       "97       2.281145  0.999976  2.446692 -2.911652 -1.582485    -100  -100.0  \n",
       "98       2.363800  0.999976  1.801015 -2.690347 -1.625902    -100  -100.0  \n",
       "99       2.675758  0.999976  2.446692 -1.805129 -0.892421    -100  -100.0  \n",
       "100      1.638564  0.999976  1.801015 -2.469043 -2.194027    -100  -100.0  \n",
       "\n",
       "[100 rows x 24 columns]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('machine').last()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
