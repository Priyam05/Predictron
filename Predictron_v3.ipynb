{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Distributions and Modules\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import distributions\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Normalization(df):\n",
    "    df.iloc[:,2:]= df.iloc[:,2:].apply(lambda x: ((x-x.mean()) / (x.std())))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(path):\n",
    "    \n",
    "    df = pd.read_csv(path, header=None, delimiter=' ')\n",
    "    \n",
    "    #Normalize the data\n",
    "    df = Normalization(df)\n",
    "    \n",
    "    #Drop the columns which has all values as Nan\n",
    "    df.dropna(axis=1, how='all', inplace=True)\n",
    "    \n",
    "    #Get Rewards for each time step : 0 except last time step where reward is -100\n",
    "    df['Counter'] = df.index\n",
    "    lastRowIndex = df.groupby(0).last().Counter.tolist()\n",
    "    df['reward'] = df['Counter'].apply(lambda x : -100 if x in lastRowIndex else 0 )\n",
    "    df.drop(columns=['Counter'],inplace=True)\n",
    "    \n",
    "    #Rename columns\n",
    "    df.rename(columns={0: \"machine\", 1: \"time\"}, inplace=True)\n",
    "    \n",
    "    #Calculate Monte Carlo Value for each row\n",
    "    df1 = df.groupby('machine').last()[['time']].reset_index()\n",
    "    df = pd.merge(df, df1, on = 'machine', how = 'left').rename(columns ={'time_x':'time','time_y':'lastTimeStamp'})\n",
    "    df['MC_Val'] = (gamma ** (df['lastTimeStamp'] - df['time'] )) * (-100)\n",
    "    df = df.drop(columns='lastTimeStamp')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = preprocessing(\"/home/abc/Berkeley/Prof_Ram/CMAPSSData/train_FD001.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We have added the reward and Val column. We will be using the Val column for the Monte Carlo return gamma**(T-t)  X  -100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now lets build the Neural network for the predictron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Neural network for Observation - Hidden State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN_OH(nn.Module):\n",
    "    def __init__(self, input_size, out_size):\n",
    "        super(NN_OH,self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size,32)\n",
    "        self.fc2 = nn.Linear(32,16)\n",
    "        self.fc3 = nn.Linear(16,out_size)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Neural network for Hidden State - Reward, Gamma, Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN_reward(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(NN_reward,self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size,32)\n",
    "        self.fc2 = nn.Linear(32,16)\n",
    "        self.fc3 = nn.Linear(16,3)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Neural network for Hidden State - Value\n",
    "class NN_val(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(NN_val,self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size,32)\n",
    "        self.fc2 = nn.Linear(32,16)\n",
    "        self.fc3 = nn.Linear(16,1)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Neural Network which will take my current hidden state to the next hidden state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN_HH(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(NN_HH,self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size,32)\n",
    "        self.fc2 = nn.Linear(32,16)\n",
    "        self.fc3 = nn.Linear(16,input_size)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we have all the required neural networks for the predictron. Lets build the Predictron\n",
    "\n",
    "class Predictronv3(nn.Module):\n",
    "    def __init__(self, obs_size, hid_size, k=10):\n",
    "        super(Predictronv3,self).__init__()\n",
    "        \n",
    "        #Instantiate Neural Network for Observation-Hidden State\n",
    "        self.OH = NN_OH(obs_size, hid_size)\n",
    "        \n",
    "        #Instantiate Neural Network for Hidden State - Reward, Value\n",
    "        self.HR = NN_reward(hid_size)\n",
    "        \n",
    "        #Instantiate Neural Network for Hidden State - Val\n",
    "        self.HV = NN_val(hid_size)\n",
    "        \n",
    "        #Instantiate Neural Network for Hidden State - Next Hidden State\n",
    "        self.HH = NN_HH(hid_size)\n",
    "        \n",
    "        #K-step return\n",
    "        self.k = k\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #Predictron core will output the value estimate for the current observation. We will input x (observation) \n",
    "        #and get value estimate. This implementation is for a k-step return which can be extended to TD(lambda) return\n",
    "        \n",
    "        #First step: Get the Hidden state for the current observation\n",
    "        \n",
    "        x = self.OH(x)\n",
    "        #Get the reward, lambda, gamma for current hidden state\n",
    "        reward = self.HR(x)[:,0].reshape(-1,1)\n",
    "        gamma = self.HR(x)[:,1].reshape(-1,1)\n",
    "        _lambda = self.HR(x)[:,2].reshape(-1,1)\n",
    "        \n",
    "        #Get value of the current hidden state\n",
    "        val = self.HV(x)\n",
    "        #print(val.shape)\n",
    "        #Get glk for 0th step\n",
    "        glk = (1-_lambda)*val + _lambda* reward\n",
    "        #print(glk.shape)\n",
    "        \n",
    "        #Store gamma and lambda as prev lambda\n",
    "        prev_gamma = gamma\n",
    "        prev_lambda = _lambda\n",
    "        \n",
    "        #Now run the loop for k steps\n",
    "        for i in range(1, self.k + 1):\n",
    "            #Move to next hidden step\n",
    "            x = self.HH(x)\n",
    "            \n",
    "            #Get the reward, lambda, gamma for current hidden state\n",
    "            reward = self.HR(x)[:,0].reshape(-1,1)\n",
    "            gamma = self.HR(x)[:,1].reshape(-1,1)\n",
    "            _lambda = self.HR(x)[:,2].reshape(-1,1)\n",
    "            \n",
    "            #Get value of the current hidden state\n",
    "            val = self.HV(x)\n",
    "            \n",
    "            #Calculate the lambda return\n",
    "            glk += (prev_gamma*prev_lambda) * ((1-_lambda)*val + _lambda* reward)\n",
    "        \n",
    "\n",
    "        return glk.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getXY(data):\n",
    "    x = torch.tensor(data.iloc[:, 2:-2].values).float()\n",
    "    y_target = torch.tensor(data.iloc[:,-1].values).float()\n",
    "    y_target = y_target.reshape(-1,1)\n",
    "    \n",
    "    return x, y_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y_target = getXY(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20631, 1])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the loss function and Initialising the Predictron core\n",
    "k=10\n",
    "loss_fn = nn.MSELoss()\n",
    "core = Predictronv3(x.shape[1], 4, k)\n",
    "optimizer = optim.Adam(core.parameters(), lr = 1e-3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Predictronv3(\n",
       "  (OH): NN_OH(\n",
       "    (fc1): Linear(in_features=21, out_features=32, bias=True)\n",
       "    (fc2): Linear(in_features=32, out_features=16, bias=True)\n",
       "    (fc3): Linear(in_features=16, out_features=4, bias=True)\n",
       "  )\n",
       "  (HR): NN_reward(\n",
       "    (fc1): Linear(in_features=4, out_features=32, bias=True)\n",
       "    (fc2): Linear(in_features=32, out_features=16, bias=True)\n",
       "    (fc3): Linear(in_features=16, out_features=3, bias=True)\n",
       "  )\n",
       "  (HV): NN_val(\n",
       "    (fc1): Linear(in_features=4, out_features=32, bias=True)\n",
       "    (fc2): Linear(in_features=32, out_features=16, bias=True)\n",
       "    (fc3): Linear(in_features=16, out_features=1, bias=True)\n",
       "  )\n",
       "  (HH): NN_HH(\n",
       "    (fc1): Linear(in_features=4, out_features=32, bias=True)\n",
       "    (fc2): Linear(in_features=32, out_features=16, bias=True)\n",
       "    (fc3): Linear(in_features=16, out_features=4, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after:0 iterations is :tensor(65.3985, grad_fn=<MseLossBackward>)\n",
      "Loss after:1 iterations is :tensor(34.8556, grad_fn=<MseLossBackward>)\n",
      "Loss after:2 iterations is :tensor(16.9897, grad_fn=<MseLossBackward>)\n",
      "Loss after:3 iterations is :tensor(31.2961, grad_fn=<MseLossBackward>)\n",
      "Loss after:4 iterations is :tensor(17.8429, grad_fn=<MseLossBackward>)\n",
      "Loss after:5 iterations is :tensor(38.1178, grad_fn=<MseLossBackward>)\n",
      "Loss after:6 iterations is :tensor(18.9797, grad_fn=<MseLossBackward>)\n",
      "Loss after:7 iterations is :tensor(25.4680, grad_fn=<MseLossBackward>)\n",
      "Loss after:8 iterations is :tensor(14.5285, grad_fn=<MseLossBackward>)\n",
      "Loss after:9 iterations is :tensor(40.9442, grad_fn=<MseLossBackward>)\n",
      "Loss after:10 iterations is :tensor(26.3427, grad_fn=<MseLossBackward>)\n",
      "Loss after:11 iterations is :tensor(21.1817, grad_fn=<MseLossBackward>)\n",
      "Loss after:12 iterations is :tensor(24.2231, grad_fn=<MseLossBackward>)\n",
      "Loss after:13 iterations is :tensor(24.7969, grad_fn=<MseLossBackward>)\n",
      "Loss after:14 iterations is :tensor(6.3854, grad_fn=<MseLossBackward>)\n",
      "Loss after:15 iterations is :tensor(22.8734, grad_fn=<MseLossBackward>)\n",
      "Loss after:16 iterations is :tensor(31.4473, grad_fn=<MseLossBackward>)\n",
      "Loss after:17 iterations is :tensor(27.2611, grad_fn=<MseLossBackward>)\n",
      "Loss after:18 iterations is :tensor(11.5874, grad_fn=<MseLossBackward>)\n",
      "Loss after:19 iterations is :tensor(40.3477, grad_fn=<MseLossBackward>)\n",
      "Loss after:20 iterations is :tensor(41.6214, grad_fn=<MseLossBackward>)\n",
      "Loss after:21 iterations is :tensor(23.8656, grad_fn=<MseLossBackward>)\n",
      "Loss after:22 iterations is :tensor(21.8795, grad_fn=<MseLossBackward>)\n",
      "Loss after:23 iterations is :tensor(22.5926, grad_fn=<MseLossBackward>)\n",
      "Loss after:24 iterations is :tensor(21.3726, grad_fn=<MseLossBackward>)\n",
      "Loss after:25 iterations is :tensor(29.1840, grad_fn=<MseLossBackward>)\n",
      "Loss after:26 iterations is :tensor(20.7218, grad_fn=<MseLossBackward>)\n",
      "Loss after:27 iterations is :tensor(28.8904, grad_fn=<MseLossBackward>)\n",
      "Loss after:28 iterations is :tensor(25.2442, grad_fn=<MseLossBackward>)\n",
      "Loss after:29 iterations is :tensor(17.1239, grad_fn=<MseLossBackward>)\n",
      "Loss after:30 iterations is :tensor(12.7368, grad_fn=<MseLossBackward>)\n",
      "Loss after:31 iterations is :tensor(27.8329, grad_fn=<MseLossBackward>)\n",
      "Loss after:32 iterations is :tensor(15.7343, grad_fn=<MseLossBackward>)\n",
      "Loss after:33 iterations is :tensor(13.4754, grad_fn=<MseLossBackward>)\n",
      "Loss after:34 iterations is :tensor(24.6633, grad_fn=<MseLossBackward>)\n",
      "Loss after:35 iterations is :tensor(13.1148, grad_fn=<MseLossBackward>)\n",
      "Loss after:36 iterations is :tensor(12.9889, grad_fn=<MseLossBackward>)\n",
      "Loss after:37 iterations is :tensor(8.4392, grad_fn=<MseLossBackward>)\n",
      "Loss after:38 iterations is :tensor(21.1989, grad_fn=<MseLossBackward>)\n",
      "Loss after:39 iterations is :tensor(17.9564, grad_fn=<MseLossBackward>)\n",
      "Loss after:40 iterations is :tensor(3.8515, grad_fn=<MseLossBackward>)\n",
      "Loss after:41 iterations is :tensor(29.6835, grad_fn=<MseLossBackward>)\n",
      "Loss after:42 iterations is :tensor(31.7025, grad_fn=<MseLossBackward>)\n",
      "Loss after:43 iterations is :tensor(37.6946, grad_fn=<MseLossBackward>)\n",
      "Loss after:44 iterations is :tensor(5.9550, grad_fn=<MseLossBackward>)\n",
      "Loss after:45 iterations is :tensor(11.9099, grad_fn=<MseLossBackward>)\n",
      "Loss after:46 iterations is :tensor(23.8409, grad_fn=<MseLossBackward>)\n",
      "Loss after:47 iterations is :tensor(9.8080, grad_fn=<MseLossBackward>)\n",
      "Loss after:48 iterations is :tensor(11.1685, grad_fn=<MseLossBackward>)\n",
      "Loss after:49 iterations is :tensor(22.1048, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 50 # or whatever\n",
    "batch_size = 256# or whatever\n",
    "losses=[]\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    # x is our input\n",
    "    permutation = torch.randperm(x.size()[0])\n",
    "\n",
    "    for i in range(0,x.size()[0], batch_size):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        indices = permutation[i:i+batch_size]\n",
    "        batch_x, batch_y = x[indices], y_target[indices]\n",
    "\n",
    "        # in case you wanted a semi-full example\n",
    "        outputs = core.forward(batch_x)\n",
    "        loss = loss_fn(outputs,batch_y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    losses.append(loss.item())\n",
    "    print(\"Loss after:\"+str(epoch)+\" iterations is :\"+ str(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Training Loss for Predictron with 10-step return')"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(losses)\n",
    "plt.xlabel(\"No. of iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss for Predictron with \"+str(k)+\"-step return\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 21])\n",
      "torch.Size([32])\n",
      "torch.Size([16, 32])\n",
      "torch.Size([16])\n",
      "torch.Size([4, 16])\n",
      "torch.Size([4])\n",
      "torch.Size([32, 4])\n",
      "torch.Size([32])\n",
      "torch.Size([16, 32])\n",
      "torch.Size([16])\n",
      "torch.Size([3, 16])\n",
      "torch.Size([3])\n",
      "torch.Size([32, 4])\n",
      "torch.Size([32])\n",
      "torch.Size([16, 32])\n",
      "torch.Size([16])\n",
      "torch.Size([1, 16])\n",
      "torch.Size([1])\n",
      "torch.Size([32, 4])\n",
      "torch.Size([32])\n",
      "torch.Size([16, 32])\n",
      "torch.Size([16])\n",
      "torch.Size([4, 16])\n",
      "torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "for p in core.parameters():\n",
    "    print(p.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = preprocessing(\"/home/abc/Berkeley/Prof_Ram/CMAPSSData/test_FD001.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test, y_test = getXY(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13096, 21])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13096, 1])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_test = core.forward(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(329.0135, grad_fn=<MseLossBackward>)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn(predict_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>...</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.730242</td>\n",
       "      <td>-0.694615</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.561019</td>\n",
       "      <td>-0.959213</td>\n",
       "      <td>-0.827618</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.175653</td>\n",
       "      <td>0.972392</td>\n",
       "      <td>-0.364968</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>-0.286877</td>\n",
       "      <td>0.540139</td>\n",
       "      <td>0.684565</td>\n",
       "      <td>-1.186405</td>\n",
       "      <td>-0.291078</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>-0.463388</td>\n",
       "      <td>0.476412</td>\n",
       "      <td>0.793594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.549865</td>\n",
       "      <td>0.325687</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>-0.062579</td>\n",
       "      <td>-1.063145</td>\n",
       "      <td>-0.100976</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.175653</td>\n",
       "      <td>-0.480742</td>\n",
       "      <td>-1.235415</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>-0.797297</td>\n",
       "      <td>-0.603480</td>\n",
       "      <td>-0.544922</td>\n",
       "      <td>-0.987163</td>\n",
       "      <td>1.029187</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>-0.463388</td>\n",
       "      <td>0.335250</td>\n",
       "      <td>0.490456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.413667</td>\n",
       "      <td>-0.014413</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>-0.885728</td>\n",
       "      <td>-0.117764</td>\n",
       "      <td>-1.442123</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.175653</td>\n",
       "      <td>1.926470</td>\n",
       "      <td>-0.539057</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>-1.154590</td>\n",
       "      <td>0.450794</td>\n",
       "      <td>-0.193640</td>\n",
       "      <td>-0.783014</td>\n",
       "      <td>-0.491013</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.347263</td>\n",
       "      <td>1.464549</td>\n",
       "      <td>-0.203788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-0.267320</td>\n",
       "      <td>-0.354514</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>-0.386850</td>\n",
       "      <td>1.237349</td>\n",
       "      <td>-0.685579</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.175653</td>\n",
       "      <td>-0.715592</td>\n",
       "      <td>-0.016789</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>-0.644171</td>\n",
       "      <td>0.558008</td>\n",
       "      <td>0.860206</td>\n",
       "      <td>-0.809514</td>\n",
       "      <td>-0.635794</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.347263</td>\n",
       "      <td>0.335250</td>\n",
       "      <td>0.729400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.276251</td>\n",
       "      <td>-1.034715</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>-0.561457</td>\n",
       "      <td>-1.133099</td>\n",
       "      <td>-0.673617</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.175653</td>\n",
       "      <td>0.003636</td>\n",
       "      <td>0.679568</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>-0.235835</td>\n",
       "      <td>0.915389</td>\n",
       "      <td>0.157642</td>\n",
       "      <td>-1.010719</td>\n",
       "      <td>-0.866755</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.347263</td>\n",
       "      <td>0.476412</td>\n",
       "      <td>0.508288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2.138835</td>\n",
       "      <td>-1.714917</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.012253</td>\n",
       "      <td>-0.963210</td>\n",
       "      <td>0.498577</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.175653</td>\n",
       "      <td>1.221920</td>\n",
       "      <td>0.157300</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>-0.440003</td>\n",
       "      <td>0.361449</td>\n",
       "      <td>-0.017999</td>\n",
       "      <td>-0.964589</td>\n",
       "      <td>-1.066690</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>-1.274039</td>\n",
       "      <td>-0.864631</td>\n",
       "      <td>0.237247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-0.312719</td>\n",
       "      <td>0.325687</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>-0.985504</td>\n",
       "      <td>-0.289651</td>\n",
       "      <td>-0.661656</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.175653</td>\n",
       "      <td>0.869645</td>\n",
       "      <td>0.331389</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>-0.388961</td>\n",
       "      <td>0.129151</td>\n",
       "      <td>-0.896204</td>\n",
       "      <td>-1.149109</td>\n",
       "      <td>0.812016</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>-0.463388</td>\n",
       "      <td>-0.158819</td>\n",
       "      <td>0.681849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.003856</td>\n",
       "      <td>1.686090</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>-1.359663</td>\n",
       "      <td>1.229354</td>\n",
       "      <td>-0.515132</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.175653</td>\n",
       "      <td>-0.289927</td>\n",
       "      <td>-0.016789</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>-1.869178</td>\n",
       "      <td>0.164889</td>\n",
       "      <td>-0.017999</td>\n",
       "      <td>-0.398270</td>\n",
       "      <td>0.729284</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>-0.463388</td>\n",
       "      <td>-0.441143</td>\n",
       "      <td>0.171865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.640663</td>\n",
       "      <td>0.325687</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>-1.309775</td>\n",
       "      <td>-0.189717</td>\n",
       "      <td>-0.990588</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.175653</td>\n",
       "      <td>0.576083</td>\n",
       "      <td>0.157300</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.019375</td>\n",
       "      <td>1.147687</td>\n",
       "      <td>-0.017999</td>\n",
       "      <td>-0.571012</td>\n",
       "      <td>-0.160085</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>-0.463388</td>\n",
       "      <td>0.899900</td>\n",
       "      <td>0.313329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-1.129903</td>\n",
       "      <td>1.345990</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.785514</td>\n",
       "      <td>-0.475529</td>\n",
       "      <td>-0.562977</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.175653</td>\n",
       "      <td>0.502692</td>\n",
       "      <td>0.331389</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>-0.082709</td>\n",
       "      <td>0.057675</td>\n",
       "      <td>0.508924</td>\n",
       "      <td>-0.408085</td>\n",
       "      <td>-0.511696</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>-1.274039</td>\n",
       "      <td>1.393968</td>\n",
       "      <td>0.845900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>-0.267320</td>\n",
       "      <td>1.345990</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.261692</td>\n",
       "      <td>-1.374941</td>\n",
       "      <td>-0.870977</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.175653</td>\n",
       "      <td>0.972392</td>\n",
       "      <td>0.157300</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>-0.950423</td>\n",
       "      <td>0.075544</td>\n",
       "      <td>-0.193640</td>\n",
       "      <td>-0.867422</td>\n",
       "      <td>-0.808153</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.347263</td>\n",
       "      <td>-0.582306</td>\n",
       "      <td>0.231303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>-0.403517</td>\n",
       "      <td>1.345990</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.461243</td>\n",
       "      <td>0.240002</td>\n",
       "      <td>0.362519</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.175653</td>\n",
       "      <td>-0.906408</td>\n",
       "      <td>0.505478</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.070417</td>\n",
       "      <td>-0.228230</td>\n",
       "      <td>1.211488</td>\n",
       "      <td>-0.918459</td>\n",
       "      <td>0.084664</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>-0.463388</td>\n",
       "      <td>0.758737</td>\n",
       "      <td>0.672339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>-0.494315</td>\n",
       "      <td>0.665788</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.087085</td>\n",
       "      <td>0.066116</td>\n",
       "      <td>0.109839</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.175653</td>\n",
       "      <td>-0.333961</td>\n",
       "      <td>-0.016789</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.172500</td>\n",
       "      <td>0.486532</td>\n",
       "      <td>0.157642</td>\n",
       "      <td>-1.854799</td>\n",
       "      <td>0.536242</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.347263</td>\n",
       "      <td>-0.370562</td>\n",
       "      <td>-0.541400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.095873</td>\n",
       "      <td>1.005889</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.261692</td>\n",
       "      <td>1.499177</td>\n",
       "      <td>0.915723</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.175653</td>\n",
       "      <td>-0.612845</td>\n",
       "      <td>1.027746</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.172500</td>\n",
       "      <td>-0.817909</td>\n",
       "      <td>0.157642</td>\n",
       "      <td>-1.197202</td>\n",
       "      <td>-1.566530</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>1.157913</td>\n",
       "      <td>0.899900</td>\n",
       "      <td>0.839956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1.140054</td>\n",
       "      <td>0.325687</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>-0.411794</td>\n",
       "      <td>-0.933230</td>\n",
       "      <td>0.522499</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.175653</td>\n",
       "      <td>0.576083</td>\n",
       "      <td>1.201836</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.019375</td>\n",
       "      <td>-0.317575</td>\n",
       "      <td>-0.193640</td>\n",
       "      <td>-0.887051</td>\n",
       "      <td>0.288046</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.347263</td>\n",
       "      <td>-0.511725</td>\n",
       "      <td>1.594827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.186672</td>\n",
       "      <td>-1.374816</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.735627</td>\n",
       "      <td>-0.613439</td>\n",
       "      <td>0.428305</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.175653</td>\n",
       "      <td>0.326555</td>\n",
       "      <td>1.027746</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.274584</td>\n",
       "      <td>-0.281837</td>\n",
       "      <td>0.684565</td>\n",
       "      <td>-1.171683</td>\n",
       "      <td>-0.039434</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>-0.463388</td>\n",
       "      <td>-0.582306</td>\n",
       "      <td>0.639053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>-0.358118</td>\n",
       "      <td>-1.034715</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.062141</td>\n",
       "      <td>-1.644764</td>\n",
       "      <td>-0.368608</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.175653</td>\n",
       "      <td>0.047670</td>\n",
       "      <td>0.679568</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.938130</td>\n",
       "      <td>0.093413</td>\n",
       "      <td>-0.193640</td>\n",
       "      <td>-0.994034</td>\n",
       "      <td>-0.263500</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>-0.463388</td>\n",
       "      <td>-0.794049</td>\n",
       "      <td>-0.795797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.867659</td>\n",
       "      <td>-0.014413</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.386412</td>\n",
       "      <td>1.235350</td>\n",
       "      <td>0.096383</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.175653</td>\n",
       "      <td>-0.289927</td>\n",
       "      <td>1.201836</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.529794</td>\n",
       "      <td>-1.389719</td>\n",
       "      <td>-0.017999</td>\n",
       "      <td>-1.165794</td>\n",
       "      <td>-0.501355</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.347263</td>\n",
       "      <td>-1.005793</td>\n",
       "      <td>2.216556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.686062</td>\n",
       "      <td>1.005889</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.336524</td>\n",
       "      <td>-0.973204</td>\n",
       "      <td>-0.023229</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.175653</td>\n",
       "      <td>-0.877051</td>\n",
       "      <td>-0.016789</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>-0.031667</td>\n",
       "      <td>-0.049540</td>\n",
       "      <td>1.387129</td>\n",
       "      <td>-0.420844</td>\n",
       "      <td>0.591397</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>-1.274039</td>\n",
       "      <td>-0.723468</td>\n",
       "      <td>-1.328368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>-0.993706</td>\n",
       "      <td>1.005889</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>-0.486625</td>\n",
       "      <td>0.128075</td>\n",
       "      <td>0.419334</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.175653</td>\n",
       "      <td>-0.744948</td>\n",
       "      <td>0.331389</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.274584</td>\n",
       "      <td>-0.478397</td>\n",
       "      <td>0.157642</td>\n",
       "      <td>-0.730013</td>\n",
       "      <td>0.691365</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>-0.463388</td>\n",
       "      <td>-1.288118</td>\n",
       "      <td>-0.745869</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          2         3         5         6         7         8         9   \\\n",
       "20  1.730242 -0.694615  0.999962  0.561019 -0.959213 -0.827618  0.999962   \n",
       "21  0.549865  0.325687  0.999962 -0.062579 -1.063145 -0.100976  0.999962   \n",
       "22  0.413667 -0.014413  0.999962 -0.885728 -0.117764 -1.442123  0.999962   \n",
       "23 -0.267320 -0.354514  0.999962 -0.386850  1.237349 -0.685579  0.999962   \n",
       "24  1.276251 -1.034715  0.999962 -0.561457 -1.133099 -0.673617  0.999962   \n",
       "25  2.138835 -1.714917  0.999962  0.012253 -0.963210  0.498577  0.999962   \n",
       "26 -0.312719  0.325687  0.999962 -0.985504 -0.289651 -0.661656  0.999962   \n",
       "27  1.003856  1.686090  0.999962 -1.359663  1.229354 -0.515132  0.999962   \n",
       "28  0.640663  0.325687  0.999962 -1.309775 -0.189717 -0.990588  0.999962   \n",
       "29 -1.129903  1.345990  0.999962  0.785514 -0.475529 -0.562977  0.999962   \n",
       "30 -0.267320  1.345990  0.999962  0.261692 -1.374941 -0.870977  0.999962   \n",
       "31 -0.403517  1.345990  0.999962  0.461243  0.240002  0.362519  0.999962   \n",
       "32 -0.494315  0.665788  0.999962  0.087085  0.066116  0.109839  0.999962   \n",
       "33  0.095873  1.005889  0.999962  0.261692  1.499177  0.915723  0.999962   \n",
       "34  1.140054  0.325687  0.999962 -0.411794 -0.933230  0.522499  0.999962   \n",
       "35  0.186672 -1.374816  0.999962  0.735627 -0.613439  0.428305  0.999962   \n",
       "36 -0.358118 -1.034715  0.999962  0.062141 -1.644764 -0.368608  0.999962   \n",
       "37  0.867659 -0.014413  0.999962  0.386412  1.235350  0.096383  0.999962   \n",
       "38  0.686062  1.005889  0.999962  0.336524 -0.973204 -0.023229  0.999962   \n",
       "39 -0.993706  1.005889  0.999962 -0.486625  0.128075  0.419334  0.999962   \n",
       "\n",
       "          10        11        12  ...        14        15        16        17  \\\n",
       "20  0.175653  0.972392 -0.364968  ...  0.999962 -0.286877  0.540139  0.684565   \n",
       "21  0.175653 -0.480742 -1.235415  ...  0.999962 -0.797297 -0.603480 -0.544922   \n",
       "22  0.175653  1.926470 -0.539057  ...  0.999962 -1.154590  0.450794 -0.193640   \n",
       "23  0.175653 -0.715592 -0.016789  ...  0.999962 -0.644171  0.558008  0.860206   \n",
       "24  0.175653  0.003636  0.679568  ...  0.999962 -0.235835  0.915389  0.157642   \n",
       "25  0.175653  1.221920  0.157300  ...  0.999962 -0.440003  0.361449 -0.017999   \n",
       "26  0.175653  0.869645  0.331389  ...  0.999962 -0.388961  0.129151 -0.896204   \n",
       "27  0.175653 -0.289927 -0.016789  ...  0.999962 -1.869178  0.164889 -0.017999   \n",
       "28  0.175653  0.576083  0.157300  ...  0.999962  0.019375  1.147687 -0.017999   \n",
       "29  0.175653  0.502692  0.331389  ...  0.999962 -0.082709  0.057675  0.508924   \n",
       "30  0.175653  0.972392  0.157300  ...  0.999962 -0.950423  0.075544 -0.193640   \n",
       "31  0.175653 -0.906408  0.505478  ...  0.999962  0.070417 -0.228230  1.211488   \n",
       "32  0.175653 -0.333961 -0.016789  ...  0.999962  0.172500  0.486532  0.157642   \n",
       "33  0.175653 -0.612845  1.027746  ...  0.999962  0.172500 -0.817909  0.157642   \n",
       "34  0.175653  0.576083  1.201836  ...  0.999962  0.019375 -0.317575 -0.193640   \n",
       "35  0.175653  0.326555  1.027746  ...  0.999962  0.274584 -0.281837  0.684565   \n",
       "36  0.175653  0.047670  0.679568  ...  0.999962  0.938130  0.093413 -0.193640   \n",
       "37  0.175653 -0.289927  1.201836  ...  0.999962  0.529794 -1.389719 -0.017999   \n",
       "38  0.175653 -0.877051 -0.016789  ...  0.999962 -0.031667 -0.049540  1.387129   \n",
       "39  0.175653 -0.744948  0.331389  ...  0.999962  0.274584 -0.478397  0.157642   \n",
       "\n",
       "          18        19        20        21        24        25  \n",
       "20 -1.186405 -0.291078  0.999962 -0.463388  0.476412  0.793594  \n",
       "21 -0.987163  1.029187  0.999962 -0.463388  0.335250  0.490456  \n",
       "22 -0.783014 -0.491013  0.999962  0.347263  1.464549 -0.203788  \n",
       "23 -0.809514 -0.635794  0.999962  0.347263  0.335250  0.729400  \n",
       "24 -1.010719 -0.866755  0.999962  0.347263  0.476412  0.508288  \n",
       "25 -0.964589 -1.066690  0.999962 -1.274039 -0.864631  0.237247  \n",
       "26 -1.149109  0.812016  0.999962 -0.463388 -0.158819  0.681849  \n",
       "27 -0.398270  0.729284  0.999962 -0.463388 -0.441143  0.171865  \n",
       "28 -0.571012 -0.160085  0.999962 -0.463388  0.899900  0.313329  \n",
       "29 -0.408085 -0.511696  0.999962 -1.274039  1.393968  0.845900  \n",
       "30 -0.867422 -0.808153  0.999962  0.347263 -0.582306  0.231303  \n",
       "31 -0.918459  0.084664  0.999962 -0.463388  0.758737  0.672339  \n",
       "32 -1.854799  0.536242  0.999962  0.347263 -0.370562 -0.541400  \n",
       "33 -1.197202 -1.566530  0.999962  1.157913  0.899900  0.839956  \n",
       "34 -0.887051  0.288046  0.999962  0.347263 -0.511725  1.594827  \n",
       "35 -1.171683 -0.039434  0.999962 -0.463388 -0.582306  0.639053  \n",
       "36 -0.994034 -0.263500  0.999962 -0.463388 -0.794049 -0.795797  \n",
       "37 -1.165794 -0.501355  0.999962  0.347263 -1.005793  2.216556  \n",
       "38 -0.420844  0.591397  0.999962 -1.274039 -0.723468 -1.328368  \n",
       "39 -0.730013  0.691365  0.999962 -0.463388 -1.288118 -0.745869  \n",
       "\n",
       "[20 rows x 21 columns]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.iloc[20:40, 2:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>...</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>reward</th>\n",
       "      <th>MC_Val</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>machine</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31</td>\n",
       "      <td>-0.267320</td>\n",
       "      <td>1.345990</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.261692</td>\n",
       "      <td>-1.374941</td>\n",
       "      <td>-0.870977</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.175653</td>\n",
       "      <td>0.972392</td>\n",
       "      <td>...</td>\n",
       "      <td>0.075544</td>\n",
       "      <td>-0.193640</td>\n",
       "      <td>-0.867422</td>\n",
       "      <td>-0.808153</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.347263</td>\n",
       "      <td>-0.582306</td>\n",
       "      <td>0.231303</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>49</td>\n",
       "      <td>0.822260</td>\n",
       "      <td>-0.354514</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.186860</td>\n",
       "      <td>-0.301643</td>\n",
       "      <td>0.911237</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.175653</td>\n",
       "      <td>-0.348639</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.013802</td>\n",
       "      <td>0.333283</td>\n",
       "      <td>-1.182480</td>\n",
       "      <td>0.849935</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>-1.274039</td>\n",
       "      <td>-0.582306</td>\n",
       "      <td>-0.879011</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>126</td>\n",
       "      <td>-0.721311</td>\n",
       "      <td>1.345990</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>1.010010</td>\n",
       "      <td>0.329943</td>\n",
       "      <td>2.116325</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.175653</td>\n",
       "      <td>-1.713704</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.639885</td>\n",
       "      <td>1.211488</td>\n",
       "      <td>-0.734921</td>\n",
       "      <td>-0.480672</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>1.968564</td>\n",
       "      <td>0.264669</td>\n",
       "      <td>-0.733981</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>106</td>\n",
       "      <td>0.549865</td>\n",
       "      <td>1.345990</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.760570</td>\n",
       "      <td>1.285318</td>\n",
       "      <td>0.320655</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.175653</td>\n",
       "      <td>-1.640314</td>\n",
       "      <td>...</td>\n",
       "      <td>0.236365</td>\n",
       "      <td>0.684565</td>\n",
       "      <td>-0.520956</td>\n",
       "      <td>1.294619</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>1.968564</td>\n",
       "      <td>-2.205674</td>\n",
       "      <td>-0.922996</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>98</td>\n",
       "      <td>-0.585114</td>\n",
       "      <td>-1.374816</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>-0.511569</td>\n",
       "      <td>0.367918</td>\n",
       "      <td>2.186597</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.175653</td>\n",
       "      <td>-0.686236</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.336111</td>\n",
       "      <td>1.387129</td>\n",
       "      <td>-1.296332</td>\n",
       "      <td>0.356990</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>1.157913</td>\n",
       "      <td>-1.005793</td>\n",
       "      <td>0.902961</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>97</td>\n",
       "      <td>-0.267320</td>\n",
       "      <td>1.005889</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>-0.436738</td>\n",
       "      <td>0.555795</td>\n",
       "      <td>-1.016006</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.175653</td>\n",
       "      <td>0.341233</td>\n",
       "      <td>...</td>\n",
       "      <td>0.986865</td>\n",
       "      <td>-1.071845</td>\n",
       "      <td>0.912017</td>\n",
       "      <td>-0.511696</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>-1.274039</td>\n",
       "      <td>0.476412</td>\n",
       "      <td>1.484271</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>134</td>\n",
       "      <td>0.595264</td>\n",
       "      <td>-0.354514</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.286636</td>\n",
       "      <td>-1.027168</td>\n",
       "      <td>0.924694</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.175653</td>\n",
       "      <td>0.429301</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.299706</td>\n",
       "      <td>-0.193640</td>\n",
       "      <td>1.622615</td>\n",
       "      <td>0.832699</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>1.968564</td>\n",
       "      <td>-1.993930</td>\n",
       "      <td>-0.480772</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>121</td>\n",
       "      <td>0.776860</td>\n",
       "      <td>0.325687</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.511131</td>\n",
       "      <td>2.280666</td>\n",
       "      <td>1.604985</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.175653</td>\n",
       "      <td>-0.466064</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.389052</td>\n",
       "      <td>0.333283</td>\n",
       "      <td>0.730442</td>\n",
       "      <td>-0.080800</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>1.157913</td>\n",
       "      <td>-0.935212</td>\n",
       "      <td>0.297875</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>97</td>\n",
       "      <td>2.138835</td>\n",
       "      <td>-0.014413</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>-1.185055</td>\n",
       "      <td>-0.613439</td>\n",
       "      <td>-1.010025</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.175653</td>\n",
       "      <td>1.456770</td>\n",
       "      <td>...</td>\n",
       "      <td>0.129151</td>\n",
       "      <td>-0.896204</td>\n",
       "      <td>1.122056</td>\n",
       "      <td>-0.880543</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>-1.274039</td>\n",
       "      <td>0.405831</td>\n",
       "      <td>0.282421</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>198</td>\n",
       "      <td>0.595264</td>\n",
       "      <td>1.005889</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>1.184617</td>\n",
       "      <td>2.702390</td>\n",
       "      <td>3.028364</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.175653</td>\n",
       "      <td>-1.875164</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.211028</td>\n",
       "      <td>-0.369281</td>\n",
       "      <td>7.429102</td>\n",
       "      <td>2.221907</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>2.779215</td>\n",
       "      <td>-1.358699</td>\n",
       "      <td>-1.786046</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         time         2         3         5         6         7         8  \\\n",
       "machine                                                                     \n",
       "1          31 -0.267320  1.345990  0.999962  0.261692 -1.374941 -0.870977   \n",
       "2          49  0.822260 -0.354514  0.999962  0.186860 -0.301643  0.911237   \n",
       "3         126 -0.721311  1.345990  0.999962  1.010010  0.329943  2.116325   \n",
       "4         106  0.549865  1.345990  0.999962  0.760570  1.285318  0.320655   \n",
       "5          98 -0.585114 -1.374816  0.999962 -0.511569  0.367918  2.186597   \n",
       "...       ...       ...       ...       ...       ...       ...       ...   \n",
       "96         97 -0.267320  1.005889  0.999962 -0.436738  0.555795 -1.016006   \n",
       "97        134  0.595264 -0.354514  0.999962  0.286636 -1.027168  0.924694   \n",
       "98        121  0.776860  0.325687  0.999962  0.511131  2.280666  1.604985   \n",
       "99         97  2.138835 -0.014413  0.999962 -1.185055 -0.613439 -1.010025   \n",
       "100       198  0.595264  1.005889  0.999962  1.184617  2.702390  3.028364   \n",
       "\n",
       "                9        10        11  ...        16        17        18  \\\n",
       "machine                                ...                                 \n",
       "1        0.999962  0.175653  0.972392  ...  0.075544 -0.193640 -0.867422   \n",
       "2        0.999962  0.175653 -0.348639  ... -0.013802  0.333283 -1.182480   \n",
       "3        0.999962  0.175653 -1.713704  ... -1.639885  1.211488 -0.734921   \n",
       "4        0.999962  0.175653 -1.640314  ...  0.236365  0.684565 -0.520956   \n",
       "5        0.999962  0.175653 -0.686236  ... -1.336111  1.387129 -1.296332   \n",
       "...           ...       ...       ...  ...       ...       ...       ...   \n",
       "96       0.999962  0.175653  0.341233  ...  0.986865 -1.071845  0.912017   \n",
       "97       0.999962  0.175653  0.429301  ... -0.299706 -0.193640  1.622615   \n",
       "98       0.999962  0.175653 -0.466064  ... -0.389052  0.333283  0.730442   \n",
       "99       0.999962  0.175653  1.456770  ...  0.129151 -0.896204  1.122056   \n",
       "100      0.999962  0.175653 -1.875164  ... -1.211028 -0.369281  7.429102   \n",
       "\n",
       "               19        20        21        24        25  reward  MC_Val  \n",
       "machine                                                                    \n",
       "1       -0.808153  0.999962  0.347263 -0.582306  0.231303    -100  -100.0  \n",
       "2        0.849935  0.999962 -1.274039 -0.582306 -0.879011    -100  -100.0  \n",
       "3       -0.480672  0.999962  1.968564  0.264669 -0.733981    -100  -100.0  \n",
       "4        1.294619  0.999962  1.968564 -2.205674 -0.922996    -100  -100.0  \n",
       "5        0.356990  0.999962  1.157913 -1.005793  0.902961    -100  -100.0  \n",
       "...           ...       ...       ...       ...       ...     ...     ...  \n",
       "96      -0.511696  0.999962 -1.274039  0.476412  1.484271    -100  -100.0  \n",
       "97       0.832699  0.999962  1.968564 -1.993930 -0.480772    -100  -100.0  \n",
       "98      -0.080800  0.999962  1.157913 -0.935212  0.297875    -100  -100.0  \n",
       "99      -0.880543  0.999962 -1.274039  0.405831  0.282421    -100  -100.0  \n",
       "100      2.221907  0.999962  2.779215 -1.358699 -1.786046    -100  -100.0  \n",
       "\n",
       "[100 rows x 24 columns]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.groupby('machine').last()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.1381e+01],\n",
       "        [-3.4868e+01],\n",
       "        [-3.8742e+01],\n",
       "        [-4.3047e+01],\n",
       "        [-4.7830e+01],\n",
       "        [-5.3144e+01],\n",
       "        [-5.9049e+01],\n",
       "        [-6.5610e+01],\n",
       "        [-7.2900e+01],\n",
       "        [-8.1000e+01],\n",
       "        [-9.0000e+01],\n",
       "        [-1.0000e+02],\n",
       "        [-8.1914e-12],\n",
       "        [-9.1015e-12],\n",
       "        [-1.0113e-11],\n",
       "        [-1.1236e-11],\n",
       "        [-1.2485e-11],\n",
       "        [-1.3872e-11],\n",
       "        [-1.5414e-11],\n",
       "        [-1.7126e-11]])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_target[180:200,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -34.8678],\n",
       "        [ -38.7421],\n",
       "        [ -43.0467],\n",
       "        [ -47.8297],\n",
       "        [ -53.1441],\n",
       "        [ -59.0490],\n",
       "        [ -65.6100],\n",
       "        [ -72.9000],\n",
       "        [ -81.0000],\n",
       "        [ -90.0000],\n",
       "        [-100.0000],\n",
       "        [  -0.6363],\n",
       "        [  -0.7070],\n",
       "        [  -0.7855],\n",
       "        [  -0.8728],\n",
       "        [  -0.9698],\n",
       "        [  -1.0775],\n",
       "        [  -1.1973],\n",
       "        [  -1.3303],\n",
       "        [  -1.4781]])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[20:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1266],\n",
       "        [-0.0077],\n",
       "        [-0.0180],\n",
       "        [-0.0393],\n",
       "        [-0.0732],\n",
       "        [-0.1652],\n",
       "        [-0.0392],\n",
       "        [-0.0406],\n",
       "        [-0.0521],\n",
       "        [-0.1707],\n",
       "        [-0.1637],\n",
       "        [-0.1978],\n",
       "        [-0.1176],\n",
       "        [-0.0929],\n",
       "        [-0.1328],\n",
       "        [-0.2042],\n",
       "        [-0.0908],\n",
       "        [-0.2661],\n",
       "        [-0.3526],\n",
       "        [-0.2334]], grad_fn=<ViewBackward>)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "core.forward(x_test[20:40,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accessing the trained Predictron Core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Predictronv2(\n",
       "  (fc1): NN_OH(\n",
       "    (fc1): Linear(in_features=21, out_features=32, bias=True)\n",
       "    (fc2): Linear(in_features=32, out_features=16, bias=True)\n",
       "    (fc3): Linear(in_features=16, out_features=4, bias=True)\n",
       "  )\n",
       "  (fc2): NN_reward_val(\n",
       "    (fc1): Linear(in_features=4, out_features=32, bias=True)\n",
       "    (fc2): Linear(in_features=32, out_features=16, bias=True)\n",
       "    (fc3): Linear(in_features=16, out_features=2, bias=True)\n",
       "  )\n",
       "  (fc3): NN_HH(\n",
       "    (fc1): Linear(in_features=4, out_features=32, bias=True)\n",
       "    (fc2): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (fc3): Linear(in_features=32, out_features=4, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.4155, -0.0080,  1.0000,  1.7179,  1.7757,  2.0294,  1.0000,  0.1417,\n",
      "        -2.3926,  3.1464, -1.4501,  1.0000,  2.6539, -1.8080,  3.1125, -1.7206,\n",
      "         1.8439,  1.0000,  1.8010, -1.8605, -3.0005]) tensor([-100.])\n"
     ]
    }
   ],
   "source": [
    "#Sample\n",
    "print(x[191],y_target[191])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 2.9286,  6.0982, -5.0383, -6.1791], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "hidden_state = core.OH(x[191])\n",
    "print(hidden_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-3.7576,  2.9589,  2.7010], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reward_sample, Reward, Gamma, Val\n",
    "core.HR(hidden_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-8.2409], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Value\n",
    "core.HV(hidden_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.5781,  3.4634, -2.7797, -1.6379], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Next hidden State\n",
    "core.HH(hidden_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-71.9248]], grad_fn=<ViewBackward>)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Net output\n",
    "core.forward(x[191].reshape(1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>...</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>reward</th>\n",
       "      <th>MC_Val</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>machine</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>192</td>\n",
       "      <td>0.415519</td>\n",
       "      <td>-0.008022</td>\n",
       "      <td>0.999976</td>\n",
       "      <td>1.717950</td>\n",
       "      <td>1.775667</td>\n",
       "      <td>2.029443</td>\n",
       "      <td>0.999976</td>\n",
       "      <td>0.14168</td>\n",
       "      <td>-2.392645</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.807964</td>\n",
       "      <td>3.112500</td>\n",
       "      <td>-1.720613</td>\n",
       "      <td>1.843870</td>\n",
       "      <td>0.999976</td>\n",
       "      <td>1.801015</td>\n",
       "      <td>-1.860455</td>\n",
       "      <td>-3.000487</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>287</td>\n",
       "      <td>-0.224536</td>\n",
       "      <td>2.039326</td>\n",
       "      <td>0.999976</td>\n",
       "      <td>2.337884</td>\n",
       "      <td>2.932057</td>\n",
       "      <td>2.433861</td>\n",
       "      <td>0.999976</td>\n",
       "      <td>0.14168</td>\n",
       "      <td>-1.929416</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.174039</td>\n",
       "      <td>1.583000</td>\n",
       "      <td>1.667906</td>\n",
       "      <td>2.515780</td>\n",
       "      <td>0.999976</td>\n",
       "      <td>3.092369</td>\n",
       "      <td>-2.137086</td>\n",
       "      <td>-1.892875</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>179</td>\n",
       "      <td>0.552673</td>\n",
       "      <td>-0.008022</td>\n",
       "      <td>0.999976</td>\n",
       "      <td>1.657956</td>\n",
       "      <td>2.328582</td>\n",
       "      <td>2.143880</td>\n",
       "      <td>0.999976</td>\n",
       "      <td>0.14168</td>\n",
       "      <td>-1.646960</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.553673</td>\n",
       "      <td>1.443954</td>\n",
       "      <td>5.849562</td>\n",
       "      <td>1.691891</td>\n",
       "      <td>0.999976</td>\n",
       "      <td>3.092369</td>\n",
       "      <td>-2.303064</td>\n",
       "      <td>-3.080856</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>189</td>\n",
       "      <td>-1.093181</td>\n",
       "      <td>-0.690471</td>\n",
       "      <td>0.999976</td>\n",
       "      <td>3.697739</td>\n",
       "      <td>3.520854</td>\n",
       "      <td>2.623848</td>\n",
       "      <td>0.999976</td>\n",
       "      <td>0.14168</td>\n",
       "      <td>-1.624363</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.133364</td>\n",
       "      <td>0.887772</td>\n",
       "      <td>6.063442</td>\n",
       "      <td>2.198489</td>\n",
       "      <td>0.999976</td>\n",
       "      <td>3.092369</td>\n",
       "      <td>-2.081760</td>\n",
       "      <td>-1.746918</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>269</td>\n",
       "      <td>1.009855</td>\n",
       "      <td>1.015652</td>\n",
       "      <td>0.999976</td>\n",
       "      <td>1.557967</td>\n",
       "      <td>1.102058</td>\n",
       "      <td>2.422750</td>\n",
       "      <td>0.999976</td>\n",
       "      <td>0.14168</td>\n",
       "      <td>-2.934961</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.106248</td>\n",
       "      <td>0.887772</td>\n",
       "      <td>3.616934</td>\n",
       "      <td>2.099836</td>\n",
       "      <td>0.999976</td>\n",
       "      <td>2.446692</td>\n",
       "      <td>-2.026434</td>\n",
       "      <td>-2.402801</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>336</td>\n",
       "      <td>-1.413208</td>\n",
       "      <td>-1.714145</td>\n",
       "      <td>0.999976</td>\n",
       "      <td>2.117907</td>\n",
       "      <td>1.415213</td>\n",
       "      <td>2.576073</td>\n",
       "      <td>0.999976</td>\n",
       "      <td>0.14168</td>\n",
       "      <td>-2.923663</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.028215</td>\n",
       "      <td>2.000136</td>\n",
       "      <td>0.014535</td>\n",
       "      <td>1.601236</td>\n",
       "      <td>0.999976</td>\n",
       "      <td>2.446692</td>\n",
       "      <td>-2.524369</td>\n",
       "      <td>-2.437905</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>202</td>\n",
       "      <td>0.644110</td>\n",
       "      <td>0.333203</td>\n",
       "      <td>0.999976</td>\n",
       "      <td>2.217897</td>\n",
       "      <td>3.284357</td>\n",
       "      <td>2.052775</td>\n",
       "      <td>0.999976</td>\n",
       "      <td>0.14168</td>\n",
       "      <td>-2.053697</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.038456</td>\n",
       "      <td>0.748727</td>\n",
       "      <td>6.284136</td>\n",
       "      <td>2.281145</td>\n",
       "      <td>0.999976</td>\n",
       "      <td>2.446692</td>\n",
       "      <td>-2.911652</td>\n",
       "      <td>-1.582485</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>156</td>\n",
       "      <td>3.524355</td>\n",
       "      <td>-1.031695</td>\n",
       "      <td>0.999976</td>\n",
       "      <td>0.678061</td>\n",
       "      <td>2.160587</td>\n",
       "      <td>2.580518</td>\n",
       "      <td>0.999976</td>\n",
       "      <td>0.14168</td>\n",
       "      <td>-2.087592</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.526556</td>\n",
       "      <td>2.278227</td>\n",
       "      <td>-1.128776</td>\n",
       "      <td>2.363800</td>\n",
       "      <td>0.999976</td>\n",
       "      <td>1.801015</td>\n",
       "      <td>-2.690347</td>\n",
       "      <td>-1.625902</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>185</td>\n",
       "      <td>-0.864590</td>\n",
       "      <td>-1.372920</td>\n",
       "      <td>0.999976</td>\n",
       "      <td>2.497867</td>\n",
       "      <td>1.287994</td>\n",
       "      <td>1.402819</td>\n",
       "      <td>0.999976</td>\n",
       "      <td>0.14168</td>\n",
       "      <td>-3.081838</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.929989</td>\n",
       "      <td>2.000136</td>\n",
       "      <td>-0.850418</td>\n",
       "      <td>2.675758</td>\n",
       "      <td>0.999976</td>\n",
       "      <td>2.446692</td>\n",
       "      <td>-1.805129</td>\n",
       "      <td>-0.892421</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>200</td>\n",
       "      <td>-1.458927</td>\n",
       "      <td>-1.714145</td>\n",
       "      <td>0.999976</td>\n",
       "      <td>2.337884</td>\n",
       "      <td>1.607673</td>\n",
       "      <td>2.578295</td>\n",
       "      <td>0.999976</td>\n",
       "      <td>0.14168</td>\n",
       "      <td>-2.912364</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.865515</td>\n",
       "      <td>2.278227</td>\n",
       "      <td>-0.336688</td>\n",
       "      <td>1.638564</td>\n",
       "      <td>0.999976</td>\n",
       "      <td>1.801015</td>\n",
       "      <td>-2.469043</td>\n",
       "      <td>-2.194027</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         time         2         3         5         6         7         8  \\\n",
       "machine                                                                     \n",
       "1         192  0.415519 -0.008022  0.999976  1.717950  1.775667  2.029443   \n",
       "2         287 -0.224536  2.039326  0.999976  2.337884  2.932057  2.433861   \n",
       "3         179  0.552673 -0.008022  0.999976  1.657956  2.328582  2.143880   \n",
       "4         189 -1.093181 -0.690471  0.999976  3.697739  3.520854  2.623848   \n",
       "5         269  1.009855  1.015652  0.999976  1.557967  1.102058  2.422750   \n",
       "...       ...       ...       ...       ...       ...       ...       ...   \n",
       "96        336 -1.413208 -1.714145  0.999976  2.117907  1.415213  2.576073   \n",
       "97        202  0.644110  0.333203  0.999976  2.217897  3.284357  2.052775   \n",
       "98        156  3.524355 -1.031695  0.999976  0.678061  2.160587  2.580518   \n",
       "99        185 -0.864590 -1.372920  0.999976  2.497867  1.287994  1.402819   \n",
       "100       200 -1.458927 -1.714145  0.999976  2.337884  1.607673  2.578295   \n",
       "\n",
       "                9       10        11  ...        16        17        18  \\\n",
       "machine                               ...                                 \n",
       "1        0.999976  0.14168 -2.392645  ... -1.807964  3.112500 -1.720613   \n",
       "2        0.999976  0.14168 -1.929416  ... -2.174039  1.583000  1.667906   \n",
       "3        0.999976  0.14168 -1.646960  ... -2.553673  1.443954  5.849562   \n",
       "4        0.999976  0.14168 -1.624363  ... -2.133364  0.887772  6.063442   \n",
       "5        0.999976  0.14168 -2.934961  ... -2.106248  0.887772  3.616934   \n",
       "...           ...      ...       ...  ...       ...       ...       ...   \n",
       "96       0.999976  0.14168 -2.923663  ... -3.028215  2.000136  0.014535   \n",
       "97       0.999976  0.14168 -2.053697  ... -2.038456  0.748727  6.284136   \n",
       "98       0.999976  0.14168 -2.087592  ... -2.526556  2.278227 -1.128776   \n",
       "99       0.999976  0.14168 -3.081838  ... -1.929989  2.000136 -0.850418   \n",
       "100      0.999976  0.14168 -2.912364  ... -2.865515  2.278227 -0.336688   \n",
       "\n",
       "               19        20        21        24        25  reward  MC_Val  \n",
       "machine                                                                    \n",
       "1        1.843870  0.999976  1.801015 -1.860455 -3.000487    -100  -100.0  \n",
       "2        2.515780  0.999976  3.092369 -2.137086 -1.892875    -100  -100.0  \n",
       "3        1.691891  0.999976  3.092369 -2.303064 -3.080856    -100  -100.0  \n",
       "4        2.198489  0.999976  3.092369 -2.081760 -1.746918    -100  -100.0  \n",
       "5        2.099836  0.999976  2.446692 -2.026434 -2.402801    -100  -100.0  \n",
       "...           ...       ...       ...       ...       ...     ...     ...  \n",
       "96       1.601236  0.999976  2.446692 -2.524369 -2.437905    -100  -100.0  \n",
       "97       2.281145  0.999976  2.446692 -2.911652 -1.582485    -100  -100.0  \n",
       "98       2.363800  0.999976  1.801015 -2.690347 -1.625902    -100  -100.0  \n",
       "99       2.675758  0.999976  2.446692 -1.805129 -0.892421    -100  -100.0  \n",
       "100      1.638564  0.999976  1.801015 -2.469043 -2.194027    -100  -100.0  \n",
       "\n",
       "[100 rows x 24 columns]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('machine').last()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
